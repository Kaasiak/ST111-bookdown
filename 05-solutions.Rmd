# (APPENDIX) Appendix {-}

# Solutions to Exercises

## Chapter\ \@ref(chapterrandomoutcomes) {-}

### Section\ \@ref(sec:equallylikely) {-}

#### Solution to Exercise\ \@ref(exr:cards40) {-}
In some cases there are multiple different, but equivalent answers. Let $A$ be the event the drawn card shows a "W" and $B$ be the event the drawn card shows a "X". 

1. The event $A\cap B$ is the card show "W" and "X", which cannot occur; that is, $A \cap B = \emptyset\subseteq \Omega$.

2. The event $A^c$ is the outcome the card does not show a "W".


#### Solution to Exercise\ \@ref(exr:dice1) {-}
In this case, $\Omega = \{1, 2, 3, 4, 5, 6\}$. Let $A$ be the event $A=\{2,4\}$ and $B=\{2,4,6\}$; that is, the dice shows an even number. Then $A\subseteq B$; that is, is the event that if the dice shows 2 or 4, then the dice shows an even number.


#### Solution to Exercise\ \@ref(exr:dice2) {-}

1. $E_1 = B \setminus C$.

2. $E_2 = C \setminus B = C \cap A$.

3. $E_3 = D \setminus A = D \cap B = D \cap C$.

4. $E_4 = A \setminus (C \cup D)$.

5. $E_5 = (B \cap C) \setminus D = C \setminus (A \cup D)$.

6. $E_6 = D \setminus B = D \setminus C = A \cap D$.


#### Solution to Exercise\ \@ref(exr:dice3) {-}
The set $\displaystyle\bigcap_{k=1}^n A_k$ is the event that every roll results is a 6. This contains one sample point, namely $\omega = (6,\ldots, 6)$ and
\[
\mathbf{P} \left(
\bigcap_{k=1}^n A_k
\right)
=
\frac{1}{6^n}.
\]


#### Solution to Exercise\ \@ref(exr:integer) {-}
The sample space is $\Omega = \{ 1, 2, \ldots, 50 \}$ and $| \Omega | = 50$. The required event is $A = \{ 12, 24, 36, 48 \}$ with $|A| = 4$. Therefore,
\[
\mathbf{P}(A) = \frac{ |A| }{|\Omega|} = \frac{4}{50} = \frac{2}{25}.
\]


### Section\ \@ref(sec:beyondevents) {-}

#### Solution to Exercise\ \@ref(exr:twoevents) {-}
This question is an application of Theorem\@ref(thm:propertiesofmeasures) and the principles used in its proof.

1. Observe that $A = (A \cap B) \cup (A \cap B^c)$ with $A\cap B$ and $A \cap B^c$ disjoint. Therefore, using additivity,
\[
\mathbf{P}( A ) = \mathbf{P} ( (A \cap B) \cup (A \cap B^c) ) = \mathbf{P}(A\cap B) + \mathbf{P}(A \cap B^c)
\]
Hence,
\[
\mathbf{P}(A \cap B) = \mathbf{P}(A) - \mathbf{P}(A \cap B^c) = \frac{1}{4} - \frac{1}{10} = \frac{15}{100}.
\]

2. Observe that $B^c = (A^c \cap B^c) \cup (A \cap B^c)$ with the union being disjoint. Then using the same methods as part 1 we have
\[
\mathbf{P}(A^c \cap B^c) = \mathbf{P}(B^c) - \mathbf{P}(A \cap B^c) = 1 - \mathbf{P}(B) - \mathbf{P} (B^c \cap A) = 1 - \frac{8}{10} - \frac{1}{10} = \frac{1}{10}.
\]


#### Solution to \@ref(exr:satprobmeasure) {-}
No. Notice that for disjoint $A$ and $B$
\[
\mathbf{P}(A \cup B) = \left(
\frac{\mbox{area}(A) + \mbox{area}(B)}{\mbox{area}(\Omega)}
\right)^2 \neq \mathbf{P}(A) +  \mathbf{P}(B)
\]


#### Solution to \@ref(exr:equallylikely) {-}
Let $A$ be an event. Then $|A| = k$ for some $k \in \{ 1, 2, \ldots n \}$ and $A$ has the form $A = \{ \omega_{i_1}, \omega_{i_2}, \ldots ,\omega_{i_k}\}$, where $1 \leq i_1 < i_2 < \ldots i_k \leq n$. Then
\[
\mathbf{P}(A) = \mathbf{P} \left( \bigcup_{j=1}^k \{ \omega_{i_j} \} \right) = \sum_{j=1}^k \mathbf{P} ( \{ \omega_{i_j} \} ) = \sum_{j=1}^k \frac{1}{n} = \frac{k}{n} = \frac{| A |}{| \Omega |}.
\]


#### Solution to \@ref(exr:twoevents2) {-}
Since $A \subseteq A \cup B$ we have $1 = \mathbf{P}(A) \leq \mathbf{P} (A \cup B)$. Since $\mathbf{P} (A \cup B) \leq 1$, if follows that $\mathbf{P}(A \cup B) =1$. Furthermore,  
\[
\mathbf{P}(A \cap B) = \mathbf{P}(A) + \mathbf{P}(B) - \mathbf{P}(A \cup B) = 1 + 1 - 1 = 1.
\]


### Section\ \@ref(sec:inclusionexclusion) {-}

#### Solution to Exercise\ \@ref(exr:tablecoin) {-}
Let $A_1$ be the the event that the coin crosses a horizontal side and $A_2$ the event that the coin crosses a vertical side. Then the probability that the coin will cross a side is $\mathbf{P}(B) = \mathbf{P}(A_1\cup A_2)$. The following diagram illustrates the sample space and the events $A_1$ and $A_2$.

```{r, out.width = "50%", fig.align='center', fig.cap = 'Geometry of the problem', echo=FALSE, fig.pos="H"} 
knitr::include_graphics('images/ST111_C1_Geometric_Probability_Example.png')
```

Note, you can compute $\mathbf{P}(A_1)$, $\mathbf{P}(A_2)$ and $\mathbf{P}(A_1\cap A_2)$ directly and the use $\mathbf{P}(A_1\cup A_2 ) = \mathbf{P}(A_1) + \mathbf{P}(A_2) - \mathbf{P}(A_1 \cap A_2)$. If you take this approach then $\mathbf{P}(A_1) = \dfrac{1}{5}$, $\mathbf{P}(A_2) = \dfrac{1}{10}$, $\mathbf{P}(A_1\cap A_2) = \dfrac{1}{50}$ and thus
\[
\mathbf{P}(A_1 \cup A_2) = \mathbf{P}(A_1) + \mathbf{P}(A_2) - \mathbf{P}(A_1 \cap A_2) = \frac{1}{5} + \frac{1}{10} - \frac{1}{50} = \frac{7}{25}. 
\]


#### Solution to Exercise\ \@ref(exr:newgame) {-}
We may assume that the numbers in the envelops are 1, 2, 3 and 4. Thus, the sample space is $\Omega = \{ (a,b,c,d) \mid a,b,c,d \in \{1,2,3,4\}\mbox{ and } a\neq b\neq c \neq d \neq a\}$. Let $(n_1,n_2,n_3,n_4)$  be such that $\{n_1,n_2,n_3,n_4\} = \{1,2,3,4\}$ and $n_k = i$ if participant $k$ receives the $i$th participant's envelop. (For example, $(2,4,1,3)$ means participant 1 gets the 2nd participant's envelope.)

Let $A_i = \{ (n_1,n_2,n_3,n_4) : n_i = i \}$. Then $A_i$ is the event that participant $i$ receives their envelope. Let 
\[
B = \bigcup_{i = 1,2,3,4} A_i.
\]
This is the event that at least one participant receives their gives, thus we shall determine $B^c$ and then (since $\Omega$ is finite)
\[
\mathbf{P}(B^c) = \frac{|\Omega| - |B|}{|\Omega|}.
\]
Note that the events $A_1$, $A_2$, $A_3$ and $A_4$ are not disjoint, so we use the inclusion--exclusion formula; that is,
\[
|B| = \left(\sum_{i=1}^4 |A_i|\right) - \left(\sum_{1\leq i< j\leq 4} |A_i \cap A_j|\right) + \left(\sum_{1\leq i_1< i_2<i_3\leq 4} |A_{i_1} \cap A_{i_2}\cap A_{i_3}|\right) - |A_1\cap A_2 \cap A_3 \cap A_4|.
\]
We proceed in stages.

- Firstly, $|A_i| = 6$. To see this note that one $n_i$ is fixed leaving 3 terms to vary freely from a selection of three numbers. In total there are 4 sets $A_i$.
- Next, for $i\neq j$, $|A_i \cap A_j | = 2$. (Both $i$ and $j$ are fixed which leaves a choice of 2 prizes for the for 2 people, but as soon as one is selected the final one is forced.). In total there are 6 sets $A_i\cap A_j$. %So by the inclusion-exclusion formula $|A_i \cup A_j| = |A_i| + |A_j| - |A_i \cap A_j| = 6+6-2 = 10$. 
- Note that for $i\neq j\neq k\neq i$ (so $k$ is not equal to $i$ either), $|A_i \cap A_j \cap A_k| = 1$; that is, three of the components of $(n_1,n_2,n_3,n_4)$ are fixed leaving one free to be chosen from a choice of 1 number. There are four sets $A_i \cap A_j \cap A_k$.
- Finally $|A_1\cap A_2 \cap A_3 \cap A_4|=1$.
- Therefore, 
\begin{align*}
|B| &= \left(\sum_{i=1}^4 |A_i|\right) - \left(\sum_{1\leq i< j\leq 4} |A_i \cap A_j|\right) + \left(\sum_{1\leq i_1< i_2<i_3\leq 4} |A_{i_1} \cap A_{i_2}\cap A_{i_3}|\right) - |A_1\cap A_2 \cap A_3 \cap A_4| \\
&= 4\times 6 - 6\times 2 + 4\times 1 - 1 \\
&= 15
\end{align*}
Therefore,  $|B^c| = 24 - 15 = 9$.

Therefore, 
\[
\mathbf{P}(B^c) = \frac{9}{24} = \frac{3}{8}.
\]


### Chapter\ \@ref(chapterrandomoutcomes) Consolidation Questions {-}

#### Solution to Exercise\ \@ref(exr:repeatedroll) {-}
The sample space is $\Omega = \{1, 2, \ldots, 6\}^n$ and contains $6^n$ sample points.
a. Let $A$ be the event that the $n$th roll produces the first 6. Then $A = \{ (\omega_1, \ldots, \omega_{n-1}, 6) \}$, where $\omega_i \in \{ 1, 2, 3, 4, 5 \}$ for $1 \leq i \leq n-1$. Then $|A| = 5^{n-1}$ and 
\[
\mathbf{P}(A) = \frac{|A|}{|\Omega|} = \frac{5^{n-1}}{6^n} = \left( \frac{5}{6} \right)^{n-1}\times\frac{1}{6}.
\] 
b. Let $A = \{ (\omega_1, \ldots, \omega_{n-1}, 6) \}$, where $\omega_i \in \{ 1, 2, 3, 4, 5 \}$ for $1 \leq i \leq n-1$ and there exists $1\leq j \leq n-1$ such that $\omega_j = 5$. Note that $|A| = |A^c|$. That is, $A$ is the event that a 5 arises before a 6, but since the die is fair, the number of ways this occurs must equal the number of ways a 6 occurs before a 5; that is, the event $A^c$. Thus since $\mathbf{P}(A) + \mathbf{P}(A^c) = 1$, $\mathbf{P}(A) = \dfrac{1}{2}$. 


#### Solution to Exercise\ \@ref(exr:fourplayers) {-}
Let $H$ denote the outcome that the coin shows a head and $T$ denote the outcome that the coin shows a tail. 

a. Note that player 1 can only win if the first two tosses are both heads. In this case the sample space is $\Omega = \{H,T\}^2$ and contains $2^2=4$ sample points. Let $A=\{(H,H)\}$ be the event that the first two coin tosses are heads, then
\[
\mathbf{P}(A) = \frac{|E|}{|\Omega|} = \frac{1}{4}.
\]
Therefore, the probability that $P_1$ wins is $\dfrac{1}{4}$.

b. (Summary solution, you should write it out formally). If the first two coin tosses are both heads, then $P_1$ must win. If the first two tosses are both tails, then $P_3$ must win. If the first two tosses are head then tail or tail then head, then either $P_2$ or $P_4$ must win. In all cases and following the same reasoning as above, each probability is $\dfrac{1}{4}$.


#### Solution to Exercise\ \@ref(exr:cardcollection) {-}
The sample space $\Omega$ is all arrangements of $1, 2, \ldots, m$; that is, $\Omega = \{ (a_1,a_2,a_3,a_4,\ldots, a_m) \mid \{a_1,a_2,a_3,a_4,\ldots, a_m\} = \{1,2,3,4,\ldots, m\} \}$. Note $|\Omega | = m!$. Introduce the notation $n_k$, where $n_k = i$ if row position $k$ has the $i$th labelled card. Let $E_i = \{ (n_1,\ldots, n_m) \mid n_i = i \}$. Then $E_i$ is the event the card labelled $i$ is in the $i$th position in the row. We wish to determine 
\[
\mathbf{P} \left( \bigcup_{1\leq i \leq m} E_i \right).
\]
Using the inclusion-exclusion formula gives
\begin{align*}
\mathbf{P} \left( \bigcup_{1\leq i \leq n} E_i \right) &= n\times \frac{1}{n} - {n \choose 2} \times \frac{1}{n(n-1)} + {n \choose 3}\times \frac{1}{n(n-1)(n-2)} - \ldots + (-1)^{m+1}\times \frac{1}{m!} \\
&= 1 - \frac{1}{2!} + \frac{1}{3!} - \ldots + (-1)^{m+1}\frac{1}{m!}.
\end{align*}
To see this in another way. The set $E_{i_1} \cap E_{i_2} \cap \ldots \cap E_{i_k}$ is the event that the cards $i_1, i_2, \ldots, i_k$ are in the correct position. By the fundamental multiplication rule for counting, this can occur in $(m-k)!$ ways. (That is, $k$ cards must return to their original positions leaving $m-k$ cards free to be placed anyway, giving $(m-k)!$ ways.) Since there are $m$ cards we have
\[
\P (E_{i_1} \cap E_{i_2} \cap \ldots \cap E_{i_k}) = \frac{(m - k)!}{m!}.
\]
Furthermore, the summation $\displaystyle \sum_{1\leq i_1 < i_2 < \ldots < i_k \leq m} \P (E_{i_1} \cap E_{i_2} \cap \ldots \cap E_{i_k})$ contains ${m \choose k}$ terms. Therefore,
\[
\sum_{1\leq i_1 < i_2 < \ldots < i_k \leq m} \P (E_{i_1} \cap E_{i_2} \cap \ldots \cap E_{i_k}) = \frac{m!}{(m-k)!k!}\times \frac{(m-k)!}{m!} = \frac{1}{k!}.
\]
So, for example, $P_1 = \frac{1}{1!}$, $P_2 = \frac{1}{2!}$ and so on.


#### Solution to Exercise\ \@ref(exr:largesquare) {-}
This question is very similar to an earlier example and an appropriate diagram is almost identical. For this reason we sketch out the solutions.

a. In this case the the centre of the coin must land in a central square with side lengths $a-2r$. Hence the probability is $\dfrac{(a-2r)^2}{a^2}$.
b. In this case determine the probability of the complementary event; that is, the coin's centre comes to rest in one of the four corner squares with side $r$. Therefore, the require probability is $1 - 4\left( \dfrac{r}{a} \right)^2$.


#### Solution to Exercise\ \@ref(exr:largesquare2) {-}
This question is very similar to an earlier example and an appropriate diagram is almost identical. For this reason we sketch out the solutions.

a. In this case the the centre of the coin must land in a central square with side lengths $a-2r$. Hence the probability is $\dfrac{(a-2r)^2}{a^2}$.

b. In this case determine the probability of the complementary event; that is, the coin's centre comes to rest in one of the four corner squares with side $r$. Therefore, the require probability is $1 - 4\left( \dfrac{r}{a} \right)^2$.


## Chapter\ \@ref(chapterenumandcount) {-}

### Section\ \@ref(sec:enumandcount) {-}

#### Solution to Exercise\ \@ref(exr:finitesets) {-}
Fix an element $x \in A$. There are $m$ choices for $f(x)$. Hence, by the fundamental multiplication rule, the total number of functions from $A$ to $B$ is $m^n$.


#### Solution to Exercise\ \@ref(exr:finitesets2) {-}

1.. Recall, $f$ is injective if $f(x) = f(y)$ implies $x=y$, or equivalently $x\neq y$ implies $f(x) \neq f(y)$. Hence, there are $m \choose n$ choices of the elements of $B$ that elements of $A$ can map to. Each selection of $n$ element of $B$ has $n!$ possible allocations of the image elements of $A$. Therefore, the number of injective functions is 
\[
{m\choose n} n!. 
\]
2. Suppose $m=n$. There are $n!$ allocations of the image elements of $A$. Therefore, the number of surjective functions from $A$ to $B$ is $n!$.


#### Solution to Exercise\ \@ref(exr:paths) {-}
1. A path is described by stating which of the 16 edges it passes along are followed upwards and which are followed rightwards, noting that there must be exactly 8 of each. So paths can be encoded by subsets of $\{1, 2, \ldots , 16\}$ having exactly 8 elements.
There are ${16 \choose 8}= 12870$ such subsets.
2. A path that passes through the centre of the board can be selected by first choosing path from the bottom/left to the centre and then choosing a path from the centre to the top/right. There are ${8 \choose 4} = 70$ possible paths for each of these choices and hence $70^2 = 4900$ paths through the centre. This gives a probability of $\dfrac{4900}{12870} \approx  0.38$.


### Section\ \@ref(sec:samples) {-}

#### Solution to Exercise\ \@ref(exr:twins) {-}
As with all questions of this type, there are many different correct solutions. A clear argument is key in all cases.

1. In this case call the teams A and B. For each pair of twins, make a choice as to which one goes into team A. The membership of team B is then forced. The total number of outcomes is $2^6$. However, we do not care which is team A and which is team B, so each possible division of the players into two sides of size 6 has happened twice (with the roles of A and B swapped). The correct answer is then $\frac{2^6}{2} = 2^5 = 32$.
2. In this case call the teams A, B and C. Suppose the teacher makes a selection for team A first. Initially there are 12 choices, once the teacher has selected one twin for team A there are 10 choices for the next member of team A, then 8 and finally 6. However, the order in which these choices are made does not matter. So, the number of ways of forming team A is
\[
\frac{12 \times 10 \times 8 \times 6}{4\times 3 \times 2 \times 1} = 240.
\]
Now, move onto team B. Once team A is formed there are four pairs of twins whose twin is already in team A and there are two pairs neither of which have been selected. To ensure that team C avoids putting any pair of twins into the same team, the teacher must select one twin from each of the two remaining pairs and 2 of the twins from the other 4 pairs. There are $2^2$ ways of selecting one twin from each of the two remaining pairs and ${4\choose 2}$ ways of selecting two twins from the other 4 pairs, so there are
\[
2^2 \times {4\choose 2} = 2^2\times 6 = 24
\]
ways to form team B. 

Once the selection for team B is made, team C is forced. 

Finally we must remember, as we did in the first part, that the team A, B and C are not specified and our method counts each division into three teams $3! = 6$ times. Therefore, the number of ways the class tutor can divide the twins into three teams of four is
\[
\frac{240\times 24}{6} = 960.
\]
(There are lots of other arguments for this question.)



#### Solution to Exercise\ \@ref(exr:gardenparty) {-}
We sketch out the key points

1. Let $T^i_1$ and $T^i_2$ denote the individuals in twin pair $i$, where $i=1,2,3$. Place $T^1_1$ in any seat, then there are $5!=120$ ways to place the remaining individuals. If each twin sits next to each other, then $T^1_2$ can sit in two seats. There are then four choices for who sits next to $T^1_2$. There are then two choices for the person opposite $T^1_1$, with the final choice forced. Therefore, there are $2\times 4\times 2 = 16$ ways for each twin to sit next to each other. Therefore, the probability is $\dfrac{16}{120} = \dfrac{2}{15}$.
2. For the moment, assume that $T^1_1$ and $T^1_2$ are not siting next to each other and the two remaining twins are sitting next to each other. Place $T_1^1$ in a seat, then to ensure the second condition there must be a gap of 2 to both the left and right of $T_1^1$; that is, $T_2^1$ is sat opposite $T_1^1$ and one pair sits to the left and the other to the right. There are two choices for how the pair of twins sat opposite are arranged. There are two choices for which pair of twins goes on the left/right. Finally, there are two choices for how an individual pair of twins arrange themselves once sat next to each other. Therefore, there are $2\times2\times 2 = 8$ ways to seat the twins with $T^1_1$ and $T^1_2$ separated, so overall there are $3\times 8=24$ ways in which one pair of twins are separated. Thus the probability is
\[
\frac{24}{120} = \frac{1}{5}.
\] 
3. There are two approaches: find the probability directly or find the probability that exactly one pair of twins sit next to each other. We have seen the complementary event used, so we compute the probability directly, you may wish to use the complementary events. If no twins sit next to each other, then there are two possible configurations, see Figure \@ref(fig:config).

```{r config, out.width = "50%", fig.align='center', fig.cap = 'Twin configuration', echo=FALSE, fig.pos="H"}
knitr::include_graphics('images/ST111_twins_configurations.png')
```

In the first case, fix one of the twins, $T_1^1$ say, then there are $2\times2\times 2= 8$ ways to arranging the twins with $T_1^1$ fixed. A different choice of $T_1^1$ just produces a rotation of one of the stated 8 configurations and thus there are no further configurations. In the second case, the cases are not symmetrical. If $T_1^1$ is in the `top' seat, then there are 8 ways to seat the other twins. Since there are three choices for which pair of twins sit opposite each other, there are $3\times 8 = 24$ arrangements. Therefore, the probability is
\[
\frac{8+24}{120} = \frac{4}{15}.
\]


#### Solution to Exercise\ \@ref(exr:internships) {-}

1. Let $\Omega = \{ 1, 2, \ldots, N\}$. Here $1$ is the highest rank and $N$ the lowest rank of the $N$ individuals after stage 1. Let $A=\{ \omega_1, \omega_2,\ldots,\omega_n\mid \omega_i = 1\mbox{ for some} i\}$ be the event that the $i$th individual in the first $n$ individuals has rank 1. There are $n$ elements in $A$, so
\[
\mathbf{P}(A) = \frac{n}{N}.
\]
2. Let $A_1$ be the event the highest ranked individual is among the first $n$ selected. Let $A_2$ be the event the second-highest ranked individual is among the first $n$ selected. Then $A_1 \cup A_2$ is the event that the highest ranked individual in this group of $n$ individuals is the overall highest or second-highest ranked from stage 1. Now $\mathbf{P}(A_1 \cup A_2) = \mathbf{P}(A_1) + \mathbf{P}(A_2) - \mathbf{P}(A_1 \cap A_2)$. Thus
\[
\mathbf{P}(A_1 \cup A_2) = \frac{{N-1 \choose n-1}}{{N \choose n}} + \frac{{N-1 \choose n-1}}{{N \choose n}} - \frac{{N-2 \choose n-2}}{{N \choose n}} = \frac{(2N - n - 1)n}{N(N-1)}.
\]


### Section\ \@ref(sec:extendedexamples) {-}

#### Solution to Exercise\ \@ref(exr:die8) {-}
In each toss of a fair die, there are 6 possible outcomes. Hence, by the fundamental multiplication rule, the total number of outcomes is equal to $6^8$. The number of outcomes that have exactly three 1’s, two 2’s and three 5’s is equal to
\[
{8 \choose 3}{5\choose 2}.
\]
Since all outcomes are equally likely, the probability of obtaining exactly three 1's, two 2's and three 5's is
\[
\frac{{8 \choose 3}{5\choose 2}}{6^8} = \frac{ \frac{8!}{3!3!2!}}{6^8}.
\]


#### Solution to Exercise\ \@ref(exr:rolldie) {-}

1. Let $\Omega$ be the sample space. Then 
\[
\Omega = \{ (i,j) : 1\leq i,j \leq 6 \} = \{ 1,2,3,4,5,6\}^2.
\]
Note that $| \Omega | = 6\times 6 = 36$. By the fundamental multiplication rule, the number of outcomes with different scores on two dice is $6\times 5$. Therefore, the probability of the event that the score in two throws are different is equal to 
\[
\frac{6\times 5}{6\times 6} = \frac{5}{6}.
\]

2. Let $\Omega$ denote the sample space. Then 
\[
\Omega = \{ (i,j,k) : 1 \leq ij,k \leq 6 \} = \{ 1,2,3,4,5,6 \}^3
\]
Note that $| \Omega | = 6^3 = 216$. By the fundamental multiplication rule, the number of outcomes with different scores on three dice is $6\times 5\times 4$. Therefore, the probability of the event that the score in two throws are different is equal to 
\[
\frac{6\times 5\times 4}{6\times 6\times 6} = \frac{5}{9}.
\]


#### Solution to Exercise\ \@ref(exr:books6) {-}
Let $A$, $B$ and $C$ be the events
\begin{align*}
A &= \{ \mbox{the two selected books are on the same subject} \}, \\
B &= \{ \mbox{the two selected books are art books} \}, \\
C &= \{ \mbox{the two chosen books are mathematics books} \}, \\
\end{align*}
Then,

1. We have
\[
\mathbf{P}(A) = \mathbf{P}(B) + \mathbf{P}(C) = \frac{ { 3\choose 2 } }{ {6 \choose 2} } + \frac{ {2\choose 2} }{ {6 \choose 2} } = \frac{4}{15}.
\]

2. Next 
\[
\mathbf{P}(A^c) = 1 - \mathbf{P}(A) = 1 - \frac{4}{15} = \frac{11}{15}.
\]


#### Solution to Exercise\ \@ref(exr:social) {-}

1. Let $A_i$, $i=1, \ldots n$ denote the event that the $i$th individual selects their own mobile phone. The event that at least one individual selects their own mobile phone is 
\[
E = \bigcup_{i=1}^{n} A_i.
\]
By the inclusion-exclusion formula
\[
\mathbf{P} \left( E = \bigcup_{i=1}^{n} A_i\right) = P_1 - P_2 + P_3 - \ldots + (-1)^{n+1}P_n
\]
where for $1\leq k \leq n$
\[
P_k = \sum_{1\leq i_1 < i_2 < \ldots < i_k \leq n} \mathbf{P} (A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_k}) 
\]
In this case, $A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_k}$ is the event that the individuals $i_1, i_2, \ldots, i_k$ select their own mobile phone. By the fundamental multiplication rule for counting, this can occur in $(n-k)!$ ways. Since there are $n$ individual we have
\[
\mathbf{P} (A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_k}) = \frac{(n - k)!}{n!}.
\]
Furthermore, the summation $\displaystyle \sum_{1\leq i_1 < i_2 < \ldots < i_k \leq n} \mathbf{P} (A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_k})$ contains $\displaystyle {n\choose k}$ terms. Therefore,
\[
\sum_{1\leq i_1 < i_2 < \ldots < i_k \leq n} \mathbf{P} (A_{i_1} \cap A_{i_2} \cap \ldots \cap A_{i_k}) = \frac{n!}{(n-k)!k!}\times \frac{(n-k)!}{n!} = \frac{1}{k!}.
\]
Therefore, $P_k = \frac{1}{k!}$ and
\[
L (n) = \mathbf{P} \left( \bigcup_{i=1}^{n} A_i\right) = P_1 - P_2 + P_3 - \ldots + (-1)^{n+1}P_n = 1 - \frac{1}{2!} + \frac{1}{3!} - \ldots + (-1)^{n+1}\frac{1}{n!}.
\]

2. Note that as $n\rightarrow \infty$, $1 - L(n) \rightarrow e^{-1}$. Therefore, $L(n) \rightarrow 1 - e^{-1}$ as $n\rightarrow \infty$. (This is approximately 0.63.)


#### Solution to Exercise\ \@ref(exr:coin100) {-}

1. We have
\[
E=\bigcup_{i=1}^{91 } A_i.
\]

2. We have $|A_i| =2^{90}$, because the results of $10$ tosses are fixed if $A_i$ occurs and $90$ remain that can be either head or tails. Thus ${\mathbf P}( A_i)= 2^{90}/ 2^{100}=(1/2)^{10}$. Then using the upper bound on the probability from the class
\[
 {\mathbf P}\left( \bigcup_{i=1}^{91} A_i\right) \leq \sum_{i=1}^{91} {\mathbf P}(A_i)
\] 
gives the desired result.


3. Note that if $i <j \leq i+10$ then $|B_i \cap B_j|=0$ because   for this event to occur the result of the $j$th toss needs to be both a head and tail. If, on the otherhand $j > i+10$, then   ${\mathbf P}( B_i \cap B_j)= (1/2)^{22}$. To make use of the lower bound from the class we need to count how many pairs  $(i,j)$ there are  with $j > i+10$.  For each $1\leq i \leq 79$ there are $80-i$ choices of $j$  ($j$ can range from $i+11$ to $90$)  giving $1+2+3+\ldots + 79= 79\times80/2= 3160$ pairs in total.  
Now 
\begin{align*}
{\mathbf P}(E) &\geq {\mathbf P}\left( \bigcup_{i=1}^{90} B_i\right) \\
&\geq \sum_{i=1}^{90} {\mathbf P}(B_i)- 
\sum_{1\leq i<j\leq 90} {\mathbf P}(B_i\cap B_j) 
\\
&=
90 \times (1/2)^{11} - 3160 \times (1/2)^{22}.
\end{align*}
which is about 0.0432.

4. The lower bound  we obtain  using $A_i$ is 
\[
{\mathbf P}(E) \geq \sum_{i=1}^{91} {\mathbf P}(A_i)- 
\sum_{1\leq i<j\leq 91} {\mathbf P}(A_i\cap A_j)
\]
But ${\mathbf P}(A_i\cap A_{i+1})= 2^{-11}$  ,  ${\mathbf P}(A_i\cap A_{i+2})= 2^{-12}$ etc, and if $j > i+9$ then ${\mathbf P}(A_i\cap A_{j})= 2^{-20}$.  There are $3321$    pairs  $(i,j)$   with $j > i+9$  so the lower bound is 
\[
91.2^{-10}- 90.2^{-11} - 89.2^{-12}- 88.2^{-13}- 87.2^{-14}- 86.2^{-15} - 85.2^{-16} -84.2^{-17}- 83.2^{-18} - 82.2^{-19}-3321.2^{-20}<0,
\]
which is useless since probabilities are always non-negative.


### Chapter\ \@ref(chapterenumandcount) Consolidation Questions {-}

#### Solution to Exercise\ \@ref(exr:smalltown) {-}
Before you sneak down to look at the answer, try this simplified version

**Simplified version**. Let's consider a simplified version that we can do directly. There are 5 voters, and the winner (W) gets 3 votes and loser (L) gets 2 votes; that is n=3 and m=2. There are ten possibilities for the order of the votes, explicitly
 
- WWWLL
- WWLLW 
- WLLWW
- LLWWW 
- WWLWL
- WLWLW 
- LWLWW 
- WLWWL
- LWWLW 
- LWWWL 
 
Now the winner must always be strictly ahead of the loser. This leaves the following possibilities WWWLL and WWLWL. This gives a probability of 2/10 or 1/5. Now, it is true that 6 of the 10 sequences start with W and only 4 start with L. But this is not what matters, it is the fact there are the same number of sequences that tie: 4 start with L and 4 start with W. It is these sequences that are equally likely to occur and help us solve this problem.

**Sketch.** A sketch answer of the main points is as follows.

1. We may consider vote counting as a path on a rectangular grid (like the chess board example) from $(0,0)$ to $(100,80)$. Label the winning candidate $A$ and the second-placed candidate $B$. Starting from $(0,0)$, each time a vote for $A$ is counted we move one step to the right and each time a vote for $B$ is counted we move one step up, eventually reaching the point (100,80). 
   Let $D = \{(i,i) \mid i \in \{1, \ldots,80\}\}$ be the set of vertices on the diagonal excluding the origin. If $A$ stays ahead of $B$ throughout the ballot counting, then the corresponding path must not include any points on or above this diagonal. We will call a path **non-favourable** if it describes a ballot counting configuration where $A$ does not stay ahead of $B$.

   Consider the first move to either $(0,1)$ or $(1,0)$. If the move is upwards to $(0,1)$ ($B$ gets a vote), then the path is non-favourable. If the move is to the right to (1,0) ($A$ gets a vote), then the path is non-favourable if in one of the following steps it visits a vertex in the set $D$. Using reflection, we can create a one-to-one correspondence between a non-favourable path that goes through $(1,0)$, say Path 1, and one that goes through $(0,1)$, call it Path 2. Let $P$ be the first vertex in $D$ visited by Path 1. Simply reflect the section of Path 1 from the origin to $P$ through the diagonal (draw a quick diagram).  As this establishes a one-to-one correspondence between non-favourable paths visiting (1,0) and paths visiting (0,1), there must be an equal number of such paths.

   The total number of paths is ${180\choose 80}$ (or ${180\choose 100}$) which are equally likely. There are ${179\choose 79}$ paths from $(0,0)$ to $(100,80)$ that go through $(1,0)$. As any non-favourable path either visits $(1,0)$ or $(0,1)$, but not both, and there are as many non-favourable paths visiting $(1,0)$ as there are visiting $(0,1)$, it follows that the probability of a non-favourable path occurring is
\begin{align*}
\mathbf{P}(\mbox{non-favourable}) &= \mathbf{P}(\mbox{non-favourable visits (0,1)}) + \mathbf{P}(\mbox{non-favourable visits (1,0)}) \\
&= 2\mathbf{P}(\mbox{non-favourable visits (1,0)}) \\
&= 2\times\frac{{179\choose 79}}{{180\choose 80}} \\
&= 2\times \frac{160}{180} = \frac{8}{9}.
\end{align*}
   Therefore, the probability that $A$ stays ahead of $B$ is $1-\dfrac{8}{9} = \dfrac{1}{9}$.
2. Using exactly the same argument and extending it to the given general cases gives the required probability as 
\[
\frac{1 - \frac{m}{n}}{1 + \frac{m}{n}}.
\]
Notice how the result from our simplified version satisfies 
\[
\frac{1 - \frac{2}{3}}{1 + \frac{2}{3} } = \frac{3 -2}{3 + 2} = \frac{1}{5}, 
\] 
which agrees with our result.

**Alternative.** Let's look at the problem on the grid, or you can just think of it abstractly. For W (winner) to be strictly ahead of L (loser) throughout the counting process there must be no ties. We know that any sequence that starts with an L must reach a tie at some point; W eventually wins. For any sequence that begins with W and reaches a tie, now switch the W and Ls to obtain a sequence that starts with an L (and hence a sequence that we know must also tie). Therefore, every sequence that begins with a W and reaches a tie is in a one-to-one correspondence (bijection) with a sequence that begins with an L. The probability that a sequence starts with a L is 
\[
\frac{m}{n + m}.
\]
As noted any sequence that starts with L must tie and corresponds to a sequence starts with W and ties. Let A be the event that a tie occurs. Then A is the disjoint union of the event that the sequence starts with W and ties (call this B) and the event that the sequence starts with L and ties (call this C). As argued above, 
\[
\mathbf{P}(B) = \mathbf{P}(C)  = \frac{m}{n + m}.
\]
Hence, the probability that W always leads is (sorry for the abused notation, forgive me)
\begin{align*}
1 - \mathbf{P}(A)  &= 1 - \mathbf{P}(B\cup C) \\
&= 1 - (\mathbf{P}(B) + \mathbf{P}(C) )  \\
&= 1 - 2 \frac{m}{n + m} \\
&= \frac{n+ m - 2m}{n + m} \\
&= \frac{n - m}{n + m}.
\end{align*}
This agrees with our generalised result. 


#### Solution to Exercise\ \@ref(exr:statdep) {-}
A sketch answer of the main points is as follows.

This is sampling without replacement. Label the row positions 1 to $n$. There are $n!$ different sequences. 

1. There are $r$ different selection for student in position 1 and $r-1$ different selections for the student in position $n$. There are $(n-2)!$ arrangements for the remaining positions from 2 to $n-1$ all of which generate equivalent arrangements. Therefore, the probability is
\[
\frac{r(r-1)(n-2)!}{n!} = \frac{r(r-1)}{n(n-1)}.
\]

2. Imagine the tennis players as one block of $r$ students. This block of $r$ leaves $n-r+1$ unallocated places (don't forget the end points, so for $n=4$, $r=2$ we have 3 places where we can insert the block of $r=2$ tennis players: either end or the middle.). There are $r!$ arrangements for the tennis players in their block. There are $(n-r+1)!$ different arrangements for the non-tennis playing students and $r!$  Therefore, the probability is
\[
\frac{(n - r + 1)!r!}{n!} = \frac{n - r + 1}{{n\choose n-r}}.
\]

3. Imagine the non-tennis players lined up next to each other. This process leaves $n-r +1$ slots into which we can insert the tennis players (don't forget the ends). We shall need to distinguish two cases, where there are sufficient gaps for the tennis players and when there are not. In order to have sufficient gaps we require $n-r+1\geq r$; that is, $n+1 \geq 2r$. If this condition fails, then there are insufficient gaps and the probability is 0. Now suppose $n+1 \geq 2r$, there are
\[
{n-r+1\choose r}
\]
ways to place the $r$ tennis players, but the tennis players can be in any one of $r!$ possible ways, giving
\[
{n-r+1\choose r}r! = \frac{(n - r + 1)!}{(n-r + 1 - r)!} = \frac{(n - r + 1)!}{(n - 2r + 1)!}
\]
Note, this is only defined for $n+1 \geq 2r$.  There are $(n-r)!$ ways to arrange the non-tennis players. Therefore, the probability is
\[
\frac{\frac{(n - r + 1)!}{(n - 2r + 1)!}\times (n-r)!}{n!} = \frac{ (n-r+1)!(n-r)!}{n!(n-2r+1)!}
\]
(Note this second probability is only defined when $n+1 \geq 2r$ as expected.)


#### Solution to Exercise\ \@ref(exr:nballs) {-}
A sketch answer of the main points is as follows.

The claim is that whichever five balls are selected, it is always possible to choose three of them so that the sum of the numbers on these three balls is a multiple of 3 and therefore the probability is 1.

Any integer has the form $3k$, $3k + 1$ or $3k + 2$, for some integer $k$; that is, any integer leaves a remainder 0, 1 or 2 when divided by 3. We know that given two integers $a$ and $b$ their sum $a+b$ is a multiple of 3 if, and only if, the sum of their remainders is divisible by 3. So, any selection of five balls will result in a collection of five different positive integers, each of these different integers will leave remainder 0, 1 or 2 when divided by 3. There are now cases to consider.

1. Suppose three of the balls have the same remainder, then their sum will be divisible by 3 and we are done. 

2. If three of the balls do not have the same remainder, then there must exist three balls with different remainders. (There can be at most two balls with identical remainder, there are five balls and three potential remainders meaning there must be at least one of each remainder.) That is, there is a pair of integers with remainder $x$, a pair of integers with remainder $y$ and a remaining integer with remainder $z$, where $x$, $y$ and $z$ are 0, 1 and 2 in some order. This means there are three balls which leave remainder 0, 1 and 2 when divide by 3 and so their sum will be divisible by 3.

Hence, in either case, we have a choice of three of the five balls who sum is a multiple of 3. Therefore, the probability is 1.


#### Solution to Exercise\ \@ref(exr:integers) {-}
We have
\begin{align*}
f(k_1) &= \frac{{n_1 \choose k_1}{N-n_2 \choose k-k_2}}{{N \choose k}} \\
&= \frac{ n_1! }{ k_1! (n_1 - k_1)! }\frac{ (n - n_1)! }{ (k - k_1)! (n - n_1 - (k - k_1)!} \frac{k! (n - k)!}{n!}\\% correct
&= {k \choose k_1} \frac{ n_1! }{ (n_1 - k_1)! }\frac{ (n - n_1)! }{ (n - n_1 - (k - k_1)!} \frac{(n - k)!}{n!}\\% correct
&= {k \choose k_1} \frac{\frac{ n_1! }{ (n_1 - k_1)! } }{\frac{n!}{(n - k_1)!}} \frac{(n - n_1)!(n - k)!}{(n - k_1)! (n - n_1 - (k - k_1))!}\\%correct
&= {k\choose k_1} \frac{\frac{ n_1! }{ (n_1 - k_1)! } }{\frac{n!}{(n - k_1)!}} \frac{ \frac{( n - n_1)!}{(n - n_1 - (k - k_1))!}}{ (n - k + (k - k_1))!}{(n - k)!}\\%complete
&= {k\choose k_1} \prod_{i=1}^{k_1} \frac{ n_1 -k + i }{n - k_1 + i} \prod_{j=1}^{k - k_1} \frac{ n - n_1 - (k - k_1) + j)}{n - k + j}\\% complete 
\end{align*}
For large $n$ with $p$ fixed, 
\[
\frac{ n_1 -k + i }{n - k_1 + i} \approx p\quad\mbox{and}\quad \frac{ n - n_1 - (k - k_1) + j)}{n - k + j} \approx 1 - p.
\]
Therefore,
\[
f(k_1) = {k\choose k_1} \prod_{i=1}^{k_1} \frac{ n_1 -k + i }{n - k_1 + i} \prod_{j=1}^{n - k_1} \frac{ n - n_1 - (k - k_1) + j)}{n - k + j}
\approx {k\choose k_1} p^{k_1} (1-p)^{k - k_1}.
\]

## Chapter \@ref(chapterindependenceandcount) {-}

### Section\ \@ref(sec:conditionalprob) {-}

#### Solution to Exercise\ \@ref(exr:balls) {-}

Let R be the event that the selected ball is red, and let Y be the event that the selected ball is yellow. We want to compute the probability of the event Y given that the event R did not occur. Hence,
\[
\mathbf{P}( R | R^c) = \frac{ \mathbf{P}( R \cap R^c )}{\mathbf{P}(R^c)} = \frac{ \mathbf{P}(R) }{ \mathbf{P}(R^c) } = \frac{ \frac{5}{30} }{ \frac{20}{30} } = \frac{5}{20} = \frac{1}{4}.
\]

#### Solution to Exercise\ \@ref(exr:myson) {-}
Let $A$ be the event the first four batteries are good and $B$ be the event the fifth battery is bad. We want to compute $\mathbf{P}( B | A) = \frac{ \mathbf{P}(A \cap B) }{\mathbf{P} (A) }.$

Computing
\[
\mathbf{P}(A \cap B) = \frac{ 10 \times 9 \times 8 \times 7 \times 3}{13\times 12 \times 11 \times 10 \times 9}.
\]
\[
\mathbf{P}(A) = \frac{ 10 \times 9 \times 8 \times 7}{13\times 12 \times 11 \times 10}.
\]
Therefore,
\[
\mathbf{P}( B | A) = \frac{ \mathbf{P}(A \cap B) }{\mathbf{P} (A) } = \frac{3}{9}.
\]

#### Solution to Exercise\ \@ref(exr:box46) {-}
Define the events:
\begin{align*}
R_i &= \{\mbox{red ball drawn in the ith draw}\},  \\
G_i &= \{\mbox{green ball drawn in the ith draw}\}.
\end{align*}

1. Computing
\[
\mathbf{P} ( (R_1 \cap G_2 ) \cup (G_1 \cap R_2 ) = \frac{6\times4}{10\times 9} + \frac{6\times 4}{10 \times 9} = \frac{48}{90} = \frac{8}{15}.
\]

2. We have
\[
\mathbf{P}(3\mbox{ draws}) = \mathbf{P}( (R_1 \cap R_2 \cap G_3) \cup (G_1 \cap G_2 \cap R_3)) = \frac{4\times 3 \times 5}{10\times 9 \times 8} + \frac{6\times 5\times 4}{10 \times 9 \times 8} = \frac{6\times 4\times 8}{10 \times 9 \times 8} = \frac{4}{15}.
\]
Next 
\[
\mathbf{P}(2\mbox{ draws}) = \mathbf{P}( (R_1 \cap G_2) \cup (G_1 \cap R_2)) = \frac{8}{15}
\]
from part 1. Hence,
\[
\mathbf{P}(\mbox{2 or 3 draws}) = \mathbf{P}(\mbox{2 draws}) + \mathbf{P}(\mbox{3 draws}) = \frac{8}{15} + \frac{4}{15} = \frac{12}{15} = \frac{4}{5}.
\]

3. We want to compute $\mathbf{P}(\mbox{3 draws} | (R_1 \cap R_2 ) \cup (G_1 \cap G_2 ))$. Note that $\{\mbox{3 draws} \} \subseteq (R_1 \cap R_2 ) \cup (G_1 \cap G_2)$.
Hence, by the definition of conditional probability,
\begin{align*}
\mathbf{P}(\mbox{3 draws} | (R_1 \cap R_2 ) \cup (G_1 \cap G_2 )) &=
\frac{ \mathbf{P}(\mbox{3 draws} \cap ( (R_1 \cap R_2 ) \cup (G_1 \cap G_2 )) ) }{\mathbf{P}( (R_1 \cap R_2 ) \cup (G_1 \cap G_2 )) } \\
&= \frac{ \mathbf{P}(\mbox{3 draws} ) }{\mathbf{P}( (R_1 \cap R_2 ) \cup (G_1 \cap G_2 )) } \\
&= \frac{ \frac{4}{15} }{\frac{4\times 3}{10\times 9} + \frac{6\times 5}{10 \times 9}} \\
&= \frac{ \frac{4}{15} }{ \frac{42}{90} } \\
&= \frac{12}{21} \\
&= \frac{4}{7}.
\end{align*}

#### Solution to Exercise\ \@ref(exr:shopkeeper) {-}
1. Let the keys be numbered from 1 to $n$. We can choose and order $k$ keys out
of $n$ in ${n \choose k} k!$ ways. In order to have first $k - 1$ keys which do not open the door and then the key that opens the door, we choose $k - 1$ keys out of $n - 1$ that do not open the door in ${n - 1 \choose k - 1}$ ways, and order them in $(k - 1)!$ ways. Furthermore we choose the key that opens the door in ${1 \choose 1}$ way.
Thus, the required probability is equal to

\[
\frac{ {n - 1 \choose k - 1} (k-1)! {1\choose 1} }{ {n \choose k }k! } = \frac{ (n - 1)! }{n!}= \frac{1}{n}.
\]

2. This is sampling with replacement and the probability is
\[
\frac{ (n - 1)^{k-1} }{n^k}.
\]

#### Solution to Exercise\ \@ref(exr:paradox) {-}
Consider a randomly chosen male applicant. Let $A_1$ be event that they apply to science, $A_2$ be the
event that they apply to humanities and $B$ be the event that their application is successful. The law of
total probability states that
\[
\mathbf{P}(B) = \mathbf{P}(A_1 ) \times \mathbf{P}(B|A_1) + \mathbf{P}(A_2 ) \times \mathbf{P}(B|A_2 )
\]
With the information that 75\% of men applied to sciences information the law of total probability reads
\[
0.6 = 0.75 \times 0.7 + 0.25 \times 0.3
\]
Only 25\% of female applicants were to Science. The corresponding law of total probability for women
reads
\[
0.5 = 0.25 \times 0.8 + 0.75 \times 0.4
\]
The explanation for the initial paradox is that women are tending to apply to the faculty which is more
selective.

### Section\ \@ref(sec:bayesandindep) {-}

#### Solution to Exercise\ \@ref(exr:cardsinbag) {-}
We sketch the answer using a variety of different approaches, you should formalise the approach that fits your particular tastes.

Let $B$ denote a black card face and $W$ a white card face. Note that the probability of selecting any card is $\dfrac{1}{3}$. Further note that the probability that the BB card has B on the other side is 1, the probability that the WW card has B on the other side is 0 and the probability that the WB card shows B is $\frac{1}{2}$.

1. Bayes' theorem implies that
\[
\mathbf{P}(BB\mid \mbox{see B}) = \frac{ \mathbf{P}( \mbox{see B} \mid BB) \times \frac{1}{3}}{\mathbf{P}(\mbox{see B}\mid BB) \frac{1}{3} + \mathbf{P}(\mbox{see B}\mid WW) \frac{1}{3} + \mathbf{P}(\mbox{see B}\mid WB) \frac{1}{3} } = \frac{1}{1 + 0 +\frac{1}{2}} = \frac{2}{3}.
\]
2. The colours are actually not important and the problem is symmetric under colour permutation. Indeed, the colour that the displayed side is the same as the hidden side is $\dfrac{2}{3}$ (WW, BB  from three cards). To formalise this approach use the law of total probability.
3. Since the card with two white faces was not drawn, the probability of seeing a black face is $\dfrac{3}{4}$ (There are four faces with three black faces). The probability of seeing a black face and having drawn the BB card is $\dfrac{1}{2}$. Therefore, the required probability is
\[
\frac{\frac{1}{2}}{\frac{3}{4}} = \frac{2}{3}.
\]
Note, this problem is famous for demonstrating difficulties with conditional probability. Countless experiments demonstrate psychological  problems processing the given information with many people providing the answer $\dfrac{1}{2}$. 


#### Solution to Exercise\ \@ref(exr:events) {-}

1. If $A$ and $B$ are disjoint, then $\mathbf{P}(A \cap B ) = 0$, but $\mathbf{P}(A)\mathbf{P}(B) > 0$. So, the events are not independent.

2. If $A$ and $B$ are independent, then $\mathbf{P}( A \cap B)= \mathbf{P}(A)\mathbf{P}(B) >0$. If $A$ and $B$ were disjoint, then $A\cap B = \emptyset$ and $\mathbf{P}(A \cap B) = 0$, which is a contradiction.


#### Solution to Exercise\ \@ref(exr:eventsabc) {-}
Computing we have
\begin{align*}
\mathbf{P}(A \cap (B \cup C)) 
&= 
\mathbf{P}( (A \cap B) \cup (A \cap C) ) \quad \mbox{Distributive law for set} \\
&= 
\mathbf{P}( A \cap B ) + \mathbf{P} (A \cap C) - \mathbf{P}(A \cap B \cap C) \quad \mbox{Using inclusion-exclsuion} \\
&= \mathbf{P}(A)\mathbf{P}(B) + \mathbf{P}(A)\mathbf{P}(C) - \mathbf{P}(A)\mathbf{P}(B)\mathbf{P}(C) \quad \mbox{Since }A, B, C \mbox{ are independent.} \\
&= \mathbf{P}(A) \left( \mathbf{P}(B) + \mathbf{P}(C) - \mathbf{P}(B)\mathbf{P}(C) \right) \\
&= \mathbf{P}(A) \left( \mathbf{P}(B) + \mathbf{P}(C) - \mathbf{P}(B \cap C) \right) \\
&= \mathbf{P}(A)\mathbf{P}(B \cup C)\quad \mbox{Using inclusion-exclsuion} 
\end{align*}

#### Solution to Exercise\ \@ref(exr:tossn) {-}
&nbsp;

1. When $n=3$ we have
\[
\mathbf{P}(A) = \mathbf{P}(\mbox{exactly two heads}) + \mathbf{P}(\mbox{exactly three heads}) = \frac{3}{8} + \frac{1}{8} = \frac{1}{2},
\]
and
\[
\mathbf{P}(B) = \mathbf{P}(\mbox{exactly one tail}) + \mathbf{P}(\mbox{exactly two tails}) = \frac{3}{8} + \frac{3}{8} = \frac{3}{4}.
\]
Furthermore,
\[
\mathbf{P}(A \cap B) = \mathbf{P}(\mbox{two heads and one tail}) = \frac{3}{8}.
\]
Since $\mathbf{P}(A)\mathbf{P}(B) = \frac{1}{2} \times \frac{3}{4} = \frac{3}{8} = \mathbf{P}(A \cap B)$, it follows that $A$ and $B$ are independent.

2.When $n=4$ we have 
\[
\mathbf{P}(A) = \mathbf{P}(\mbox{exactly two heads}) + \mathbf{P}(\mbox{exactly three heads}) + \mathbf{P}(\mbox{exactly four heads})= \frac{6}{16} + \frac{4}{16} + \frac{1}{16}= \frac{11}{16},
\]
and
\[
\mathbf{P}(B) = \mathbf{P}(\mbox{exactly one tail}) + \mathbf{P}(\mbox{exactly two tails}) = \frac{4}{16} + \frac{6}{16} = \frac{10}{16} = \frac{5}{8}.
\]
Further,
\[
\mathbf{P}(A \cap B) = \mathbf{P}(\mbox{three heads and one tail}) + \mathbf{P}(\mbox{two heads and two tails})= \frac{4}{16} + \frac{6}{16} = \frac{5}{8}.
\]
Since $\mathbf{P}(A)\mathbf{P}(B) = \frac{11}{16} \times \frac{5}{8} \neq \frac{5}{8} = \mathbf{P}(A \cap B)$, it follows that $A$ and $B$ are not independent.

#### Solution to Exercise\ \@ref(exr:suitable) {-}
We need to show 

1. for all $A$ for which $\mathbf{P}_B$ is defined, $0\leq \mathbf{P}_B(A) \leq a$ and that $\mathbf{P}_B(\emptyset) = 0$ and $\mathbf{P}_B(\Omega) = 1$.

2. For a finite or infinite sequence of mutally disjoint events $A_1$, $A_2$, \ldots
\[
\mathbf{P}_B\left( \bigcup_{i=1}^n A_i \right) = \sum_{i=1}^n \mathbf{P}_B(A_i).
\]
with the similar statement for a countably infinite sequence.

1. Let $A$ be an event. Then $A\cap B$ is also an event and $\mathbf{P}(A \cap B)\geq 0$. Hence,
\[
\mathbf{P}_B (A) = \mathbf{P}(A|B) = \frac{ \mathbf{P}(A \cap B) }{\mathbf{P}(B)} \geq 0.
\]
Since $A \cap B \subset B$, it follows since $\mathbf{P}$ is a probability measure that $\mathbf{P}(A \cap B) \leq \mathbf{P}(B)$. Hence, $\mathbf{P}_B (A) \leq 1$. Further
\[
\mathbf{P}_B (\Omega) = \mathbf{P}(\Omega|B) = \frac{ \mathbf{P}(\Omega \cap B) }{\mathbf{P}(B)} = \frac{ \mathbf{P}( B) }{\mathbf{P}(B)} = 1.
\]
Finally, 
\[
\mathbf{P}_B (\emptyset) = \mathbf{P}(\emptyset|B) = \frac{ \mathbf{P}(\emptyset \cap B) }{\mathbf{P}(B)} = \frac{ \mathbf{P}( \emptyset) }{\mathbf{P}(B)} = 0.
\]
This verifies the conditions in part 1.

2. (We prove the finite case directly which generalises to the infinite case by using countable additivity. ) Let $A_1, A_2, \ldots$ be a sequence of mutally disjoint events. Then 
\begin{align*}
\mathbf{P}_B \left( \bigcup_{i=1}^n A_i \right) 
&= 
\mathbf{P} \left( \bigcup_{i=1}^n A_i \mid B \right) \\
&=
\frac{ \mathbf{P}\left( (\bigcup_{i=1}^n A_i) \cap B  \right )}{\mathbf{P}(B) } \\
&=
\frac{ \mathbf{P}\left( (\bigcup_{i=1}^n A_i \cap B)  \right )}{\mathbf{P}(B) } \\
\end{align*}
Since the events $A_i$, $i= 1, \ldots n$ are disjoint, it follows that $(A_i \cap B)$ $i= 1, \ldots n$ are disjoint. Hence, by additivity
\[
\mathbf{P}\left( \bigcup_{i=1}^n (A_i \cap B) \right)= \sum_{i=1}^n \mathbf{P}(A_i \cap B),
\]
which implies
\[
\mathbf{P} \left( \bigcup_{i=1}^n A_i \mid B \right) = \frac{\sum_{i=1}^n \mathbf{P}(A_i \cap B) }{\mathbf{P}(B)} = \sum_{i=1}^n \frac{ \mathbf{P}(A_i \cap B)}{\mathbf{P}(B)}.
\]
Since $\mathbf{P}(B)>0$ we can use the definition of conditional probability to get
\[
\mathbf{P}_B \left( \bigcup_{i=1}^n A_i \right) 
=
\mathbf{P}\left( \bigcup_{i=1}^n A_i \mid B \right)
=
\sum_{i=1}^n \frac{ \mathbf{P}(A_i \cap B)}{\mathbf{P}(B)}
=
\sum_{i=1}^n \mathbf{P}(A_i|B)
=
\sum_{i=1}^n \mathbf{P}_B(A_i).
\]
where we have used the defintion of conditional probability $\frac{ \mathbf{P}(A_i \cap B)}{\mathbf{P}(B)} = \mathbf{P}(A_i|B)$.

### Section\ \@ref(sec:indepapplications) {-}

#### Solution to Exercise\ \@ref(exr:biasgame) {-}
Consider the experiment where the coin is tossed $n$ times. Let $\Omega \subseteq \mathbf{Z}^n$ be such that each $\omega \in \Omega$ is a sequence of integers that record how the counter moves. For example, if $n=4$, then $(1,2,3,4)$ is an element of $\Omega$. Let $X^n_p$ be the event that $\omega_n = p$; that is, after $n$ tosses the counter is located at $p$. That is,  $X_n = \{ \omega = (\omega_1, \ldots, \omega_{n-1}, p ) \}$. 

1. We require $\mathbf{P}(X^4_{-2})$ that is the probability that after 4 steps the counter finishes at $-2$. Starting at $0$ to finish at $-2$ in 4 moves requires 3 negative steps and 1 positive step; that is, $-2 = -1 - 1 -1 + 1$, with this the unique decomposition of $-2$ into four unit integers. There are ${4 \choose 1}$ ways to achieve this, therefore
\[
\mathbf{P}(X^4_{-2}) = {4 \choose 1}pq^3.
\]

2. We require $\mathbf{P}(X^6_{-3})$. Let $i\in\mathbf{Z}$ be the number of "heads" and $j\in\mathbf{Z}$ be the number of "tails". The total number of moves is 6, so $i+j=6$. Since each "head" results in a move $n\mapsto n+1$ and each "tail" produces a move $n\mapsto n-1$ and the counter must finish on $-3$ we also have $i-j = -3$. But there are no integer solutions, so $\mathbf{P}(X^6_{-3})=0$. (Note in general making an even number of moves cannot result in the counter finishing on an integer with different parity.) 

Note, once we can talk about random variables, this question's notation becomes easier. 

#### Solution to Exercise\ \@ref(exr:biasgame) {-}
Consider the experiment where the coin is tossed $n$ times. Let $\Omega \subseteq \mathbf{Z}^n$ be such that each $\omega \in \Omega$ is a sequence of integers that record how the counter moves. For example, if $n=4$, then $(1,2,3,4)$ is an element of $\Omega$. Let $X^n_p$ be the event that $\omega_n = p$; that is, after $n$ tosses the counter is located at $p$. That is,  $X_n = \{ \omega = (\omega_1, \ldots, \omega_{n-1}, p ) \}$. 

1. We require $\mathbf{P}(X^4_{-2})$ that is the probability that after 4 steps the counter finishes at $-2$. Starting at $0$ to finish at $-2$ in 4 moves requires 3 negative steps and 1 positive step; that is, $-2 = -1 - 1 -1 + 1$, with this the unique decomposition of $-2$ into four unit integers. There are ${4 \choose 1}$ ways to achieve this, therefore
\[
\mathbf{P}(X^4_{-2}) = {4 \choose 1}pq^3.
\]

2. We require $\mathbf{P}(X^6_{-3})$. Let $i\in\mathbf{Z}$ be the number of "heads" and $j\in\mathbf{Z}$ be the number of "tails". The total number of moves is 6, so $i+j=6$. Since each "head" results in a move $n\mapsto n+1$ and each "tail" produces a move $n\mapsto n-1$ and the counter must finish on $-3$ we also have $i-j = -3$. But there are no integer solutions, so $\mathbf{P}(X^6_{-3})=0$. (Note in general making an even number of moves cannot result in the counter finishing on an integer with different parity.) 

Note, once we can talk about random variables, this question's notation becomes easier. 

#### Solution to Exercise\ \@ref(exr:gambler) {-}

1. The event $F_{m,k}$ is not quite the complement of $E_{m,k}$ because the wealth of the gambler could in principle oscillate forever between the values of $1$ and $m - 1$.

2. Note that the probability the gambler could in principle oscillate forever between the values of $1$ and $m - 1$ has
probability zero.
   Since $\mathbf{P}(F_{m,k}) + \mathbf{P}(E_{m , k}) = 1$, and
\begin{equation}
\mathbf{P}(F_{m,k}) =
\left\{
\begin{array}{lcl}
\dfrac{m-k}{m} & & p = \frac{1}{2}, \\
&& \\
\dfrac{\left( \frac{1-p}{p}\right)^m - \left( \frac{1-p}{p} \right)^k}{\left( \frac{1-p}{p}\right)^m -1} & & \mbox{otherwise}
\end{array}
\right.
(\#eq:GRruin)
\end{equation}

3. The relation $F_{m,k} \subseteq F_{m+1,k}$ holds because if a gambler is ruined before their wealth ever reaches $m$
then they necessarily are ruined before reaching a wealth of $m + 1$. 

4. Let
\[
F_k = \bigcup_{m\geq k} F_{m,k},
\]
this is the event that gambler is _ever_ ruined if they play indefinitely. 

5. Using $\mathbf{P}(F_k ) = \lim_{m\rightarrow \infty} \mathbf{P}(F_{m,k})$, and taking limits in \@ref(eq:GRruin) gives
\[
\mathbf{P}(F_k ) =
\left\{
\begin{array}{lcl}
1 & & p \leq \frac{1}{2}, \\
&& \\
\left( \frac{1-p}{p} \right)^k & & p > \frac{1}{2}
\end{array}
\right.
\]
That is, in a game that is fair ($p=\frac{1}{2}$) or bias against the gambler, the probability of being ruinned if they play indefinitely is 1.


### Chapter \@ref(chapterindependenceandcount) Consolidation Questions {-}


#### Solution to Exercise\ \@ref(exr:genes) {-}

1. Let $\Omega = \{(1,1), (1,2), (2,1), (2,2) \}^4$ where $(i,j)$ denotes the gene types for the first and second gene, respectively. Each element $\omega\in \Omega$ provides the gene type for one of the four children. For example, $( (1,1), (1,1), (1,2), (2,2))$ states the first and second child both have both genes of type 1 an so on. Note that $|\Omega| = 256 = 4^4$. Let $A$ be the event that two children both have two genes of type 1 and two children have two genes of type 2. The number of elements in $A$ is
\[
\frac{4!}{2!2!} = 6.
\] 
Therefore, 
\[
\mathbf{P}(A) = \frac{6}{256} = \frac{3}{128}.
\]

2. Label the parents $P_1$ and $P_2$ with $P_1$ the parent we know has one gene of type 1 and one gene of type 2. Let $B_{ij}$ be the event that parent $P_2$ has one gene of type $i$ and one gene of type $j$. Let $C$ be the event that the child has two genes of type 1. Using Bayes' formula (and simplifying the conditional probabilities) gives
\[
\mathbf{P}(B\mid C) = \frac{ \mathbf{P} ( B_{12} \cap C) }{\mathbf{P}( B_{11} \cap C) + \mathbf{P}( B_{12} \cap C) } = \frac{0.18 \times 0.25}{0.18\times 0.25 + 0.81\times 0.5} = \frac{1}{10}.
\]

#### Solution to Exercise\ \@ref(exr:clothing) {-}
The following sketches the key points from a potential solution.

1. The probability is
\[
\frac{1}{7} 
\left( \frac{1}{3} \times \frac{1}{3} + \frac{1}{3} \times \frac{1}{3} + \frac{1}{3} \times \frac{1}{4} \right) + \frac{6}{7} \left( \frac{1}{3} \times \frac{1}{3} + \frac{1}{3} \times \frac{1}{3} + \frac{1}{3} \times \frac{1}{2} \times \frac{1}{4} \right) = \frac{22}{504} + \frac{114}{504}.
\]
the first bracket being the probability of wearing the black shirt on Sunday. We now require Bayes' Theorem
\[
P(A|B) = \frac{P(A\cap B)}{P(B)}.
\]
So the probability that it is Sunday, given that I am wearing my black shirt, is therefore by Bayes' Theorem
\begin{eqnarray*}
P(\mbox{Sunday}|\mbox{Wearing black shirt})
&=& 
\frac{P( \mbox{Sunday and wearing black shirt})}{P(\mbox{wearing black shirt})} \\
&=& \frac{\frac{22}{504}}{ \frac{22}{504} + \frac{114}{504}} \\
&=& \frac{22}{22+114} \\
&=& \frac{11}{68}.
\end{eqnarray*}

2. For the second part, consider Mondays separately. The new probability that I am wearing my black shirt is (Monday represented by the last bracket)
\[
\frac{1}{7} \left( \frac{1}{3} \times \frac{1}{3} + \frac{1}{3} \times \frac{1}{3} + \frac{1}{3} \times \frac{1}{4} \right) + \frac{5}{7} 
\left( \frac{1}{3} \times \frac{1}{3} + \frac{1}{3} \times \frac{1}{3} + \frac{1}{3} \times \frac{1}{2} \times \frac{1}{4} \right) + \frac{1}{7} \left( \frac{1}{2} \times \frac{1}{4}\right) = \frac{22}{504} + \frac{95}{504} + \frac{9}{504}.
\]
The probability that it is Sunday, given that I am wearing my black shirt, is now (using Bayes' Theorem again in the same manner as above. 
\[
\frac{22}{22+95+9} = \frac{11}{63}.
\]

#### Solution to Exercise\ \@ref(exr:randomsol) {-}
There are multiple solutions to this problem.

1. Let $\Omega$ be the sample space. This consists of all triples $(x,y,z)$ that satisfy $x+y+z=20$. Firstly, count the total number of solutions; that is $|\Omega|$. For a given $x$, the solution pairs $(y,z)$ have the form $(0,20-x), (1,20-x-1),\ldots, (20-k,0)$. That is, there are $21-x$ different solutions for a fixed $x$. But $0\leq x \leq 20$, each of which generates $21-x$ solutions. This gives a total of
\[
\sum_{i=0}^{20} (21 - i) = 21\times 11 = 231
\]
solutions. So $|\Omega| = 231$.

Let $X_1$ be the event that $x$ is divisible by 5. Suppose $x$ is divisible by 5, then $x=0,5,10,15,20$. The number of solutions for which $x=5$ is then 
\[
(21 - 0) + (21 - 5) + (21 - 10) + (21 - 15) + (21 - 20) = 55.
\]
and 
\[
\mathbf{P}(X_1) = \frac{55}{21\times 11} = \frac{5}{21}.
\]

2. No they are not independent event, we have seen that $y$'s value depends on $x$'s. Or to put it another way, the probability that $y=20$ given that we know $x=20$ is zero, which is not the same as the probability that $y=20$ in any randomly selected solution.

#### Solution to Exercise\ \@ref(exr:gameround) {-}
Let $\Omega=\{ n \in \mathbf{Z}\mid n\geq 0\}$. Let $E_n$ be the event you reach $n$ points starting with 0 points. Following the arguments for the gambler's ruin problem gives
\[
\mathbf{P}(E_n)  = \frac{1}{2}\mathbf{P}(E_{n-1}) + \frac{1}{2}\mathbf{P}(E_{n-2})
\]
Let $p(n) = \mathbf{P}(E_n)$, then we get $p_n - \frac{1}{2}p_{n-1} - \frac{1}{2}p_{n-2} = 0$. Using the method given in the notes gives the general solution
\[
p(n) = C_1 + C_2\left( - \frac{1}{2} \right)^n.
\]
The situations permits determination of $p(1)$ and $p(2)$. Note that $p(1) = \dfrac{1}{2}$. Have a score of 2 arises in two ways, either you score 2 points directly, or you score 1 point twice. Thus, $p(2) = \dfrac{1}{2} + \dfrac{1}{2}\times\dfrac{1}{2} = \dfrac{3}{4}$. Using these conditions gives $C_1= \dfrac{2}{3}$ and $C_2 = \dfrac{1}{3}$. Therefore,
\[
p(n) = \frac{2}{3} + \frac{1}{3}\left( - \frac{1}{2} \right)^n
\]
Note that for large $n$, $p(n) \approx \dfrac{2}{3}$, what do you think this means?


## Chapter\ \@ref(chapterdistributions) {-}

### Section\ \@ref(sec:binomial) {-}

#### Solution to Exercise\ \@ref(exr:seeds) {-}

1. Let $X$ denote the number of seeds out of 6 seeds planted that will germinate.
We want to compute $\mathbf{P}(X \geq 2)$. Observe that $X$ has binomial distribution with parameters $n=6$ and $p = \dfrac{2}{3}$. Hence,
\[
\mathbf{P}(X = k) = {6 \choose k}\left( \frac{2}{3} \right)^k\left( \frac{1}{3} \right)^{6 - k}.
\]
for $k=0,\ldots 6$. It follows that
\begin{align*}
\mathbf{P}(X \geq 2) &= 1 - \mathbf{P}(X = 0) - \mathbf{P}(X = 1) \\
&= 1 - \left( \frac{1}{3} \right)^6 - 6 \left( \frac{2}{3} \right)\left( \frac{1}{3} \right)^5 \\
&= 1 - \frac{1}{3^6} - \frac{12}{3^6} \\
&= \frac{716}{729}
\end{align*}

2. By the definition of conditional probability and by additivity of conditional probabilities 
\[
\mathbf{P}(X \leq  5 |X \geq 3) = 1 - \mathbf{P}(X = 6 |X \geq 3) = 1 - \frac{\mathbf{P}(X = 6, X \geq 3)}{\mathbf{P}(X \geq 3)} =
1 - \frac{ \mathbf{P}(X = 6)}{\mathbf{P}(X \geq 3)}.
\]
Since

\begin{align*}
\mathbf{P}(X \geq 3) &= \mathbf{P}(X = 3) + \mathbf{P}(X = 4) + \mathbf{P}(X = 5) + \mathbf{P}(X = 6) \\
&=
{6 \choose 3} \left( \frac{2}{3} \right)^3 \left( \frac{1}{3} \right)^3
+
{6 \choose 4} \left( \frac{2}{3} \right)^4 \left( \frac{1}{3} \right)^2
+
{6 \choose 5} \left( \frac{2}{3} \right)^5 \left( \frac{1}{3} \right)^1
+
\left( \frac{2}{3} \right)^6 \\
&= \frac{656}{3^6}.
\end{align*}
it follows that
\begin{align*}
\mathbf{P}(X \leq 5 | X \geq 3) &= 1 - \frac{ \mathbf{P}(X = 6) }{\mathbf{P}(X \geq 3) } \\
&= 1 - \frac{ \left( \frac{2}{3} \right)^6 }{ \frac{656}{3^6} } \\
&= 1 - \frac{64}{656} \\
&= 1 - \frac{4}{41} \\
&= \frac{37}{41}.
\end{align*}

#### Solution to Exercise\ \@ref(exr:tables) {-}
Let $X$ be a random variable that denotes the number of people out of 52 who reserved a table at the restaurant and do not show up. Then $X$ is a binomial random variable with parameters $n=52$ and $p=0.2$. The restaurant will be able to accommodate everyone if two or more people who reserved a table at the restaurant do not show up, that is if $\{X \geq 2\}$. Hence, the probability that the restaurant will be able to accommodate everyone is equal to
\[
\mathbf{P}(X \geq 2) = 1 - \mathbf{P}(X = 0) - \mathbf{P}(X = 1) = 1 - (0.8)^{52} - 52\times 0.2 \times (0.8)^{51}.
\]


### Section\ \@ref(sec:wlln) {-}

#### Solution to Exercise\ \@ref(exr:gamblingvariant) {-}
Let $X$ be the number of times we win in $n$ games then our final wealth is given by 
\[
(1 + 2\alpha)^X (1 - \alpha)^{n - X} = (1 - \alpha)^n \left( \frac{1 + 2 \alpha}{1 - \alpha} \right)^X.
\]
We know from the weak law of large numbers that for any $\varepsilon > 0$, with probability tending to one that $X$ belongs to the interval $[n(p - \varepsilon), n(p + \varepsilon)]$. Consequently with high probability the wealth will be close to
\[
(1 + 2 \alpha)^{np} (1 - \alpha )^{n (1 - p)}
\]
Thus to maximize the growth rate we need to maximize 
\[
(1 + 2 \alpha)^{p} (1 - \alpha )^{(1 - p)}
\]
Taking logarithms and looking for a turning point gives the condition
\[
\frac{2p}{1 + 2\alpha} - \frac{1-p}{1-\alpha} = 0.
\]
For $p = 2/3$, the unique solution (and hence the unique turning point, which must be a maximum) is $\alpha= 1/2$.

### Section\ \@ref(sec:wlln-proof) {-}

#### Solution to Exercise\ \@ref(exr:unitsquares) {-}
1. Each route from $(0,0)$ to $(n,n)$ travels along $2n$ edges, exactly $n$ of these are parallel to the $x$ (or $y$) axis. Hence, from $2n$ edges we must choose $n$, giving ${2n \choose n}$ routes.

2.
    (i) Starting from $(0,0)$ we move to $(k, n-k)$ and then move from $(k, n - k)$ to $(n,n)$. Using the same reasonsing as part (1), there are ${n \choose k}$ routes from $(0,0)$  to $(k, n-k)$ and then ${n \choose k}$ routes from $(k, n - k)$ to $(n,n)$. The multiplication rule implies there are ${n \choose k}^2$. 
    (ii) Forming the sum over $k$ gives the total number of routes from $(0,0)$ to $(n,n)$, therefore, using the result from part (i), we get
    \[
\sum_{k=0}^{n}\left(\frac{n !}{k !(n-k) !}\right)^{2} = {2n \choose n}.
\]
3. Suppose $E_{n,k}$ is the event that a random path on the $n\times n$ board passes through $(k,n-k)$. Then 
\[
E_n = \bigcup_{|2k -n|<n/100} E_{n,k}.
\]
For fixed $k$ such that $|2k -n|<n/100$; that is, $\frac{n}{2} - \frac{n}{200} < k < \frac{n}{2} + \frac{n}{200}$ we have
\[
\mathbf{P}(E_{n,k}) = \dfrac{{n\choose k}^2}{{2n \choose n}}.
\]
So,
\[
\mathbf{P}(E_n) = \sum_{\frac{n}{2} - \frac{n}{200} < k < \frac{n}{2} + \frac{n}{200}} \dfrac{{n\choose k}^2}{{2n \choose n}}. 
\]
But using the notation from the question we can rewrite this expression so that we can use the bound given in the question. In particular,
\[
f(k,n,1/2)^2 = \left( {n\choose k}\left( 1/2 \right)^k \left( 1/2 \right)^{n-k} \right)^2
= {n\choose k}^2 \left( 1/2 \right)^{2n}
\]
and
\[
f(n,2n,1/2) = {2n\choose n} \left( 1/2 \right)^{2n}.
\]
Hence, we want to compute
\[
\mathbf{P}(E_n) = \frac{1}{f(n;2n,\frac{1}{2})} \sum_{\frac{n}{2} - \frac{n}{200} < k < \frac{n}{2} + \frac{n}{200}} f\left(k;n,\frac{1}{2}\right)^2 
\]
Using the complementary event and the given result $\dfrac{f(k ; n, p)}{f(n ; 2 n, p)} \leq M$ for all $n \geq 1$, $p=1/2$ and $0 \leq k \leq n$, we have
\[
\mathbf{P}(E_n^c) = \frac{1}{f(n;2n,\frac{1}{2})} \sum_{\begin{array}{c} 
k\leq\frac{n}{2} - \frac{n}{200}\\ k \geq \frac{n}{2} + \frac{n}{200}
\end{array}} f\left(k;n,\frac{1}{2}\right)^2.
\]
Using the given bound we get
\[
\mathbf{P}(E_n^c) = \frac{1}{f(n;2n,\frac{1}{2})} \sum_{\begin{array}{c} 
k\leq\frac{n}{2} - \frac{n}{200}\\ k \geq \frac{n}{2} + \frac{n}{200}
\end{array}} f\left(k;n,\frac{1}{2}\right)^2
\leq 
M \sum_{\begin{array}{c} 
k\leq\frac{n}{2} - \frac{n}{200}\\ k \geq \frac{n}{2} + \frac{n}{200}
\end{array}} f\left(k;n,\frac{1}{2}\right)
\]
Note that $f\left(k;n,\frac{1}{2}\right) = \mathbf{P}(X = k)$ where $X$ is a Binomial random variable with parameters $p=1/2$ and $n$. Hence,
\[
\sum_{\begin{array}{c} 
k\leq\frac{n}{2} - \frac{n}{200}\\ k \geq \frac{n}{2} + \frac{n}{200}
\end{array}} f\left(k;n,\frac{1}{2}\right)
=
\mathbf{P}(X \leq \frac{n}{2} - \frac{n}{200}) + \mathbf{P}(X \geq \frac{n}{2} + \frac{n}{200}).
\]
By Weak Law of Large Numbers with $p = \frac{1}{2}$ and $\varepsilon = \frac{1}{200}$ we have $\mathbf{P}(E_n^c) \rightarrow 0$ as $n$ tends to infinity. Therefore, $\mathbf{P}(E_n) \rightarrow 1$ as $n$ tends to infinity.

**Commentary on the question** Part 3 is challenging, so here is a commentary on the question.

We recall some facts from the notes

- $\mathbf{P}(X=k) = {n \choose k} p^k (1-p)^{n-k}$ where $X$ is a Binomial random variable with parameters $n$ and $k$.
- We have $\mathbf{P}(a \leq X \leq b) = \sum_{k=a}^b \mathbf{P}(X=k)$ where $0 \leq a \leq b \leq b$ and $a,b$ are integers.
- The statement in the WLLN that 
\[
\mathbf{P}(np - n\varepsilon \leq X_n \leq np + n\varepsilon) = \sum_{np - n\varepsilon \leq k \leq np + n\varepsilon,\ k\in \mathbf{Z}} {n \choose k} p^k (1-p)^{n-k}.
\]

Part 3 asks us to evaluate 
\[
\lim_{n\rightarrow \infty} \mathbf{P}(E_n).
\]
Before we can do this we need to determine an expression for $\mathbf{P}(E_n)$. Once we have an expression we need to determine how best to compute the limit.The first part of the question generates an expression for $\mathbf{P}(E_n)$ where we note that we are summing over $k$s for which
$n/2 - n/200 < k < n/2 + n/200$. This looks very much like an expression that the WLLN generates. The hint from the question then shows us we can bound  $\mathbf{P}(E_n)$ by an expression that is exactly like that seen in the WLLN. However, we know from our work in Analysis that if we are going to evaluate such a limit then we need inequality in the form
\[
a_n \leq b_n \leq c_n
\]
with the limits of $(a_n)$ and $(c_n)$ being identical. We note that if we move to the complementary event, then we get
\[
0\leq \mathbf{P}(E_n^c) \leq M \sum_{\begin{array}{c} 
k\leq\frac{n}{2} - \frac{n}{200}\\ k \geq \frac{n}{2} + \frac{n}{200}
\end{array}} f\left(k;n,\frac{1}{2}\right).
\]
It is only at this point that we apply the WLLN to show that $\mathbf{P}(E_n^c)$ tends to 0 using the squeeze rule from analysis. Hence, we must have $\mathbf{P}(E_n)$ tends to 1. 

### Section\ \@ref(sec:poisson) {-}

#### Solution to Exercise\ \@ref(exr:radioactive) {-}
Divide the minute into intervals of length $\delta t$ seconds. Then we expect that when $\delta t$ is small
the probability of one or more emissions occurring in the interval is approximately $\delta t/10$. Note the
probability of more than one emission is negligible as $\delta t$ becomes small. Assume the independence
of emissions in the different intervals. Then the total number of emissions in the minute should be
approximately Binomially distributed with parameters $n=60/(\delta t)$ and $p = (\delta t)/10$. That is, there are $n = 60/(\delta t)$ intervals in one minute and the probability of any event in one of those intervals is $p = (\delta t)/10$. But $\delta t$ is arbitrary and can be taken very small so that $n$ is very large, and then the Poisson distribution for the number of emissions is reasonable. Hence, every one minute there are $np = 6$ events, but this must also equal $\lambda\times 1 = 6$. (Where the 1 is represening 1 minute.) Therefore, a Poisson distribution with $\lambda = 6$ (per minute) is appropriate.

#### Solution to Exercise\ \@ref(exr:randomvar) {-}
Recall that $e^{\lambda} = \sum_{k=0}^{\infty} \frac{\lambda^k}{k!}$, hence
\[
\sum_{k=0}^{\infty} \mathbf{P}(X=k) = \sum_{k=0}^{\infty} e^{-\lambda} \frac{\lambda^k}{k!} = e^{-\lambda} \sum_{k=0}^{\infty}  \frac{\lambda^k}{k!} = e^{-\lambda}e^{\lambda} = 1.
\]

#### Solution to Exercise\ \@ref(exr:online) {-}
1. 
  (i) The probability of winning $k$ times is given by a Binomial random variable $X$ with parameters $n=52$ and $p = \frac{1}{104}$. Thus
  \[\mathbf{P}(X = k) = p(k,52, 1/104) = {52 \choose k} \left( \frac{1}{104} \right)^k  \left( 1 - \frac{1}{104} \right)^{52 - k}.\]
    
  (ii) We use the Poisson distribution to form the approximation. In particular, using the parameter $\lambda = np = 52\times \frac{1}{2\times 52}$ we get 
  \[\mathbf{P}(X = k) \approx \frac{ (1/2)^k}{k!}e^{-\frac{1}{2}}. \]
  
  (iii) With $k=2$, we get the probability of winning twice is approximately
  \[ \frac{ (1/2)^2}{2!}e^{-\frac{1}{2}} = \frac{1}{8\sqrt{e}}. \]
  
  (iv) Since $\mathbf{P}(X \geq 2) = 1 - \mathbf{P}(X=0)- \mathbf{P}(X=1)$ we have the probability of winning at least twice is
  \begin{align*} 
  1 - \frac{ (1/2)^0}{0!}e^{-\frac{1}{2}} - \frac{ (1/2)^1}{1!}e^{-\frac{1}{2}}  &= 1 - e^{-\frac{1}{2}} - \frac{1}{2}e^{-\frac{1}{2}} \\
  &= 1 - \frac{3}{2\sqrt{e}}.
  \end{align*}
    
2. In this case we again use the Poisson distribution to approximate the Binomial distribution. In particular, using this approximation we know by generalising part (iv) that as $n\rightarrow \infty$ the probability of winning at least twice is
\[
1 - \frac{ (\lambda)^0}{0!}e^{-\lambda} - \frac{ \lambda}{1!}e^{-\lambda} = 1 - (1 + \lambda)e^{-\lambda}.
\]
Now we know that $p = \frac{1}{n}$ and the number of times played is at least $2n$, hence $\lambda \geq 2n \times \frac{1}{n} = 2$. Hence, since the function $\lambda \mapsto 1 - (1 + \lambda)e^{-\lambda}$ is increasing, we have
\[
1 - (1 + \lambda)e^{-\lambda} \geq 1 - (1 + 2)e^{-2} = 1 - 3e^{-2}.
\]
Using the given fact, $e^2 > 6$ we have that $-e^{-2} > 1/6$. Hence, $1 - (1 + \lambda)e^{-\lambda} \geq  1 - 3e^{-2} > 1 - 3\times \frac{1}{6} = 1/2$. Applying the definition of limit we know that there exists an integer $N\geq 1$ such that if $n\geq N$ then $\mathbf{P}(X \geq 2) \geq 1/2$.

### Section\ \@ref(sec:gaussian) {-}

#### Solution to Exercise\ \@ref(exr:students) {-}
Consider a random variable $X$ having the Binomial distribution with parameters $n = 400$ and $p = \dfrac{1}{2}$. Of course $\mathbf{P}(X = 230)$ is small. But the Gaussian approximation to the Binomial distribution gives
\[ 
\mathbf{P}(X \geq  230) \approx \int_{z_1}^{\infty} \phi(z)dz 
\]
To compute $z_1$ use the statement of the De Moivre-Laplace theorem, that is, $230 = np + z_1\sqrt{np(1-p)}$, so
\[
230 = 400\times \frac{1}{2} + z_1 \sqrt{400 \times \frac{1}{2}\times \frac{1}{2}}.
\]
Thus $z_1 = 3$. Therefore,
\[ 
\mathbf{P}(X \geq  230) \approx \int_{3}^{\infty} \phi(z)dz = 0.001\ ( 3 d.p.),
\]
and on this basis we view it as very unlikely we would have observed as many as birthdays as we did falling in the first six months if the uniform model was a good one.

Here, the theorem tells us nothing about how good the approximation we have obtained will be. But in fact these approximations are very good indeed.

#### Solution to Exercise\ \@ref(exr:cricket) {-}
Assume the hits are independent and that the probability of a hit is $p=0.3$. Model the situation as a binomial random variable $X_n$ with parameters $n=535$ and $p=0.3$. We want to know $\mathbf{P}(X_n \geq 197)$. Using the DeMoivre-Laplace theorem
\[
\mathbf{P}(X \geq  197) \approx \int_{z_1}^{\infty} \phi(z)dz 
\]
where $z_1$ satisfies $535\times 0.3 + z_1 \sqrt{535\times 0.3\times 0.7} = 197$. That is, 
\[
z_1 = \frac{197 - 535\times 0.3}{\sqrt{535\times 0.3\times 0.7}} = 3.444\mbox{ (3 decimal place)}.
\]
A computation (using, for example, (http://onlinestatbook.com/2/calculators/normal_dist.html) ) gives $\mathbf{P}(X \geq  197) \approx 0.0003$.

### Chapter \@ref(chapterdistributions) Consolidation Questions {-}

#### Solution to Exercise\ \@ref(exr:printing) {-}
1. Let $A$ be the event that the reader spots $m$ misprints and fails to spot $n$ misprints, then 
\[
\mathbf{P}(A) = {m+n \choose m}p^m (1-p)^n.
\]
That is, for a fixed ordering of the $m+n$ possible trials the probability of $m$ success and $n$ failures is $p^m (1-p)^n$, but we need to include the number of ways this even can occur.

2. Let B be the event that the reader fails to spot exactly $n$ misprint. Let $C$ be the event the reader spots $m$ misprints. Then
\[
\mathbf{P}(B\mid C) = \frac{\mathbf{P}(B\cap C)}{\mathbf{P}(C)}. 
\]
Now,
\[
\mathbf{P}(B \cap C) = \frac{e^{-\lambda}\lambda^{n+m}}{(m+n)!}p^m (1-p)^n \frac{ (n+m)!}{m!n!},
\]
and
\[
\mathbf{P}(C) = \sum_{r=0}^{\infty} \frac{e^{-\lambda}\lambda^{r+m}}{(r+m)!} p^m (1-p)^r {m+r \choose m} = \frac{e^{-\lambda}p^m\lambda^m}{m!} \sum_{r=0}^{\infty} \frac{\lambda^r (1-p)^r}{r!}.
\]
Therefore,
\[
\mathbf{P}(B\mid C) = \frac{\lambda^n (1-p)^n }{n!} \left(\sum_{r=0}^{\infty} \frac{\lambda^r (1-p)^r}{r!}\right)^{-1} = \frac{\lambda^n (1-p)^n }{n!} e^{-(1-p)\lambda}.
\]

#### Solution to Exercise\ \@ref(exr:posters) {-}
1. Let $X$ be the random variable that denotes the number of errors spotted. Then
\begin{align*}
\mathbf{P}(X=k) &= \sum_{i=k}^{\infty} \frac{e^{-\lambda}\lambda^i}{i!} \frac{i!}{k!(i-k)!}p^k (1-p)^{i-k} \\
&= \frac{e^{-\lambda}\lambda^i p^k}{k!} \sum_{i=k}^{\infty} \frac{ \lambda^{i-k} (1-p)^{i-k}}{(i-k)!} \\
&= \frac{e^{-\lambda}\lambda^i p^k}{k!}e^{\lambda (1-p)} \\
&= \frac{e^{-p\lambda}(p\lambda)^k}{k!}.
\end{align*}

2. Let $A_1$ be the event that you place a red cross on one of the first $k-1$ posters and $A_2$ be the event that you place no red crosses on one of the first $k-1$ posters. Let $X$ be the random variable denoting the number of red crosses placed on the $k$th poster. We require
\[
\mathbf{P}(A_1)\mathbf{P}(X\geq 1) + \mathbf{P}(A_2)\mathbf{P}(X\geq 2).
\]
Note $\mathbf{P}(X\geq 1) = 1 - \mathbf{P}(X=0) = 1 - e^{-p\lambda}$ and $\mathbf{P}(X\geq 2) = 1 - \mathbf{P}(X=0) - \mathbf{P}(X=1) = 1 - e^{-p\lambda} (1 + p\lambda)$. Next
\[
\mathbf{P}(A_1) = {k-1 \choose 1} e^{-p\lambda} p\lambda (e^{-p\lambda})^{k-2} = (k-1)p\lambda e^{-(k-1)p\lambda}.
\] 
Finally
\[
\mathbf{P}(A_2) = e^{-(k-1) p \lambda}.
\]
Therefore,
\begin{align*}
\mathbf{P}(A_1)\mathbf{P}(X\geq 1) + \mathbf{P}(A_2)\mathbf{P}(X\geq 2) &= (k-1)p\lambda e^{-(k-1)p\lambda} \times (1 - e^{-p\lambda}) + e^{-(k-1) p \lambda} \times (1 - e^{-p\lambda} (1 + p\lambda)) \\
&= e^{-p\lambda(k-1)}\left( (k-1)p\lambda(1 - e^{-p\lambda}) + [ 1- e^{-p\lambda}(1+p\lambda)] \right) \\
&= e^{-p\lambda(k-1)}\left( (1 - e^{-p\lambda})[ (k-1)p\lambda + 1 + p\lambda]  - p\lambda   \right) \\
&= e^{-p\lambda(k-1)}\left[ (1 - e^{-p\lambda}) (kp\lambda + 1) - p\lambda \right].
\end{align*}

#### Solution to Exercise\ \@ref(exr:binomialdist) {-}
This question utilises the methods from an earlier example.

We sketch this answer following the exact method from the example. Apply the De Moivre-Laplace noting that $z_1 = -\dfrac{1}{\sqrt{2n}}$ and $z_2 = \dfrac{1}{\sqrt{2n}}$. So,
\[
\mathbf{P}(X=n) \approx \int_{-\frac{1}{\sqrt{2n}}}^{\frac{1}{\sqrt{2n}}} \dfrac{1}{\sqrt{2\pi}} e^{-z^2/2} dz.
\]
Approximate this integral as a rectangle of height $\dfrac{1}{\sqrt{2\pi}}$ and width $\sqrt{\dfrac{2}{n}}$ gives
\[
\mathbf{P}(X=n) \approx \dfrac{1}{\sqrt{n\pi}}.
\]
Hence
\[
\mathbf{P}(X=n) = \frac{ (2n)! }{n!n!} \left( \frac{1}{2} \right)^{2n} \approx \dfrac{1}{\sqrt{n\pi}}.
\]
Rearranging gives the result.

#### Solution to Exercise\ \@ref(exr:score1) {-}
Let $\Omega=\{ n \in \mathbf{Z}\mid n\geq 0\}$. Let $E_n$ be the event you reach $n$ points starting with 0 points. Following the arguments for the gambler's ruin problem gives
\[
\mathbf{P}(E_n)  = \frac{1}{2}\mathbf{P}(E_{n-1}) + \frac{1}{2}\mathbf{P}(E_{n-2})
\]
Let $p(n) = \mathbf{P}(E_n)$, then we get $p_n - \frac{1}{2}p_{n-1} - \frac{1}{2}p_{n-2} = 0$. Using the method given in the notes gives the general solution
\[
p(n) = C_1 + C_2\left( - \frac{1}{2} \right)^n.
\]
The situations permits determination of $p(1)$ and $p(2)$. Note that $p(1) = \dfrac{1}{2}$. Have a score of 2 arises in two ways, either you score 2 points directly, or you score 1 point twice. Thus, $p(2) = \dfrac{1}{2} + \dfrac{1}{2}\times\dfrac{1}{2} = \dfrac{3}{4}$. Using these conditions gives $C_1= \dfrac{2}{3}$ and $C_2 = \dfrac{1}{3}$. Therefore,
\[
p(n) = \frac{2}{3} + \frac{1}{3}\left( - \frac{1}{2} \right)^n
\]
Note that for large $n$, $p(n) \approx \dfrac{2}{3}$, what do you think this means?
