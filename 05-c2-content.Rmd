\clearpage
## Class 2: Beyond equally likely events

We have described how probabilities are defined on a sample space when its appropriate to model all outcomes as being equally likely---uniform probabilities. We look to extend this idea to other situations.

### Motivating examples 


:::: {.example}
Suppose a population consists of $n$ individuals, each of whom has one of $k$ possible types. There are
$n_i$ individuals of type $i$ with $n = n_1 + n_2 + \ldots + n_k$. If an individual is chosen uniformly at random then the probability they are of type $i$ is given by the proportion $p_i = \dfrac{n_i}{n}$.
::::


:::{.solution}
We may only be concerned with observing the type of the individual chosen and thus represent the experiment with the sample space $\Omega = \{1 , 2 , \ldots , k \}$, where the sample point $i$ corresponds to the outcome that the chosen individual is of type $i$. On this
sample space we should not use uniform probabilities but rather if $A\subseteq \Omega$  then the probability that $A$
occurs is the proportion of the population that have the corresponding types and
\[
\mathbf{P}(A) = \sum_{\omega_i \in A} p_i.
\]
For example, suppose we have 1000 individuals, with $126$ of type 1, $357$ of type 2 and $517$ of type 3. Then the probability of $A=\{1,2\}$ is
\[
\mathbf{P}(A) = \frac{126}{1000} + \frac{357}{1000} = \frac{483}{1000}.
\]
:::

:::: {.example}
Suppose we break an (idealised) stick of length $L$ into two pieces by snapping at a position $x$ measured from a fixed end. We can
take as the sample space $\Omega = \{x\in \mathbf{R} : 0  \leq x  \leq L\}.$ Pick positions $a$ and $b$ with $0  \leq a \leq b \leq L$. Some motivational questions are

1. What should the probability be that the break occurs between $a$ and $b$?

2. Determine the probability of snapping the stick exactly in half.

3. Suppose we snap the stick at two points. How could we model this?
::::


:::{.solution}
&nbsp;

1. If we break the stick uniformly at random then this probability could be modelled as being given by the proportion of the total stick length that lies between $a$ and $b$, so if $A = [a, b]$  then
\[
\mathbf{P}(A) = \frac{\mbox{length}(A)}{L}.
\]

2. By taking $a = b$ in the previous formula we deduce that $\mathbf{P}(\{a\}) = 0$ for every $a$. 

3.  One reasonable possibility (there are many others!) is to take the sample space to be $\Omega= \{(x, y)\in  \mathbf{R}^2 : 0\leq x\leq L,\ 0\leq y\leq L\}$, with probabilities given by
\[
\mathbf{P}(A) = \frac{\mbox{area}(A)}{L^2}.
\]
:::

### Probability measures 

Notice that in all our examples probabilities have been given by proportions, and given by real
numbers between 0 and 1. In general probabilities are assigned to events. The stick breaking example
shows it's not enough to just consider the probabilities of sample points. The question that arises is:


_What properties must a function $A\mapsto \mathbf{P}(A)$ possess in order for it to be meaningfully be able to be interpreted as specifying probabilities?_


For example, if   $A\subseteq\Omega \subseteq \mathbf{R}^2$, would the function
\[
\mathbf{P}(A) = \left(
\frac{\mbox{area}(A)}{\mbox{area}(\Omega)}
\right)^2
\]
make sense as probabilities, where the areas are defined?

The next definition formalises the properties we expect.

:::{.definition}
A _probability measure_ on a sample space $\Omega$ is a real-valued function $A\mapsto \mathbf{P}(A)$ defined
on a collection of subsets of $\Omega$ that satisfies the following properties.

1. For all $A$, for which $\mathbf{P}(A)$ is defined, $0\leq \mathbf{P}(A) \leq 1$. Moreover $\mathbf{P}(\emptyset) = 0$ and $\mathbf{P}(\Omega) = 1$.

2. (Additivity) For a finite or infinite sequence of mutually disjoint events $A_1, A_2 , \ldots,$
\[
\mathbf{P} \left( \bigcup_{i=1}^n A_i\right) = \sum_{i=1}^n \mathbf{P} (A_i),
\]
and, in the case of an infinite sequence,
\[
\mathbf{P} \left( \bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} \mathbf{P} (A_i).
\]
in the finite and infinite cases respectively.
:::

Recall, the sets $A_1, A_2, \ldots A_n , \ldots$ are mutually disjoint if $A_i  \cap A_j = \emptyset$  whenever $i \neq j$. 

(We provides some additional comments on this definition in the class footnotes, specifically regarding the definition^[**Comments on the definition of probability measure** For finite or countably infinite $\Omega$, probabilities will be defined on all subsets of $\Omega$; but this will not be so for uncountably infinite sample spaces. Indeed length, area and volume cannot be defined for all subsets of $\mathbf{R}$, $\mathbf{R}^2$ and $\mathbf{R}^3$ without generating paradoxes. Sets for which length, area or volume can be defined are called _measurable_ and includes all sets that you will come across in practice.If these comments interest you, then look up the Banach--Tarski theorem, von Neumann paradox or a Vitali set.] and additivity^[**Some comments on additivity** For a finite or countably infinite sample space additivity implies, by forming
a sequence from the sample points, that the probability of any event is the sum of the probabilities of the sample points it contains. For an uncountably infinite sample space we cannot make the same argument because we cannot make a sequence of all the sample points in a general event. Additivity for infinite sequences of events is very useful in uncountable sample spaces though. For example it allows us to argue that the area of a disc can be calculated by covering the disc with an infinite sequence of rectangles of decreasing size, and computing the sum of their areas. As an example of additivity failing to hold we can consider the following attempt to define probabilities:
\[
\mathbf{P}(A) = \lim_{n\rightarrow \infty} \frac{1}{n} |A \cap \{1, 2, \ldots , n\}|
\]
for any $A\subseteq \mathbf{N}$ for which the limit exists. This looks like it might define appropriate probabilities for choosing a random number `uniformly from $\mathbf{N}$'--- but such a notion is not useful. Indeed, if we could define such an idea, then if $B_i=\{i\}$, then $\mathbf{P}(B_i)=\mathbf{P}(B_j)$ for any $i$, $j$ (the probability is meant to be uniform). Specifically, $\mathbf{P}(B_i) = \mathbf{P}(B_1)$. Suppose $\mathbf{P}(B_1) =p > 0$. Then for some natural number $n$, additivity implies the probability of the event  that a number between 1 and $n$ is selected must exceed $1$. (Eventually, $np>1$ no matter how small $p$ is.)].)


:::{.example}
The sample point $(x,y)\in \Omega$ corresponds to the outcome in which the dart lands at the point $(x,y)$. Probabilities are assigned to events so that they are proportional to the area of the corresponding subset of $\Omega$.  Determine the probability that the point lies in one of the four regions outside the circle and inside the square.
::::

:::{.solution}
A diagram is presented on Figure \@ref(fig:sqcircle).

```{r sqcircle, out.width = "30%", fig.align='center', fig.cap = 'Geometry of circle and square', echo=FALSE} 
knitr::include_graphics('images/ST111_C1_Geometric_B.png')
```

Let $L$ be the side length of the square. Let $\Omega=\{ (x,y) \in \mathbf{R}^2 \mid 0\leq x \leq L \mbox{ and } 0 \leq y \leq L\}$. Probabilities are assigned to events so that they are proportional to the area of the corresponding subset. Let $A$ be the event $\left\{ (x,y) \in \Omega \mid (x - \frac{L}{2})^2 + (y - \frac{L}{2})^2 \geq \frac{L^2}{4} \right\}$. That is, $A$ is the set of points in the four shaded regions. Then
\[
\mathbf{P}(A) = \frac{ \mbox{Area A} }{\mbox{Area }\Omega} = \frac{L^2 - \pi \left(\frac{L}{2}\right)^2}{L^2} = 1 - \frac{\pi}{4}.
\]
:::


:::{.theorem #propertiesofmeasures}
For any probability measure the following properties hold. (We assume the probabilities are defined.)

1. For any $A$, $\mathbf{P}(A^c) = 1 - \mathbf{P}(A)$.

2. If $A\subseteq B$ then $\mathbf{P}(A) \leq \mathbf{P}(B)$.

3. For any two events $A$ and $B$, not necessarily disjoint,
\[
\mathbf{P}(A \cup B) = \mathbf{P}(A) + \mathbf{P}(B) - \mathbf{P}(A \cap B).
\]

4. For a general sequence of events $A_1 , A_2 , \ldots,$ not necessarily disjoint,
\[
\mathbf{P} \left( \bigcup_{i=1}^n A_i \right) \leq \sum_{i=1}^n \mathbf{P} (A_i),
\]
and, 
\[
\mathbf{P} \left( \bigcup_{i=1}^{\infty} A_i \right) \leq \sum_{i=1}^{\infty} \mathbf{P} (A_i).
\]
for finite and infinite sequences respectively.
:::

:::{.proof}
&nbsp;

1. Note that $A$ and $A^c$ are disjoint, $A\cap A^c = \emptyset$ and $A\cup A^c = \Omega$. By additivity $\mathbf{P}(A^c ) + \mathbf{P}(A) = \mathbf{P}(\Omega)$, and this latter probability is equal to 1 by definition of a probability measure.

2. If $A \subseteq B$ then we can write $B = A \cup (B \cap A^c )$ and since $A$ and $B\cap A^c$ are disjoint we have by
additivity that $\mathbf{P}(B) = \mathbf{P}(A) + \mathbf{P}(B \cap A^c )$. The desired inequality follows since $P(B \cap A^c ) \geq 0$.

3. Let $A_1 = A \cap B^c$ , $B_1 = B \cap  A^c$ and $C = A \cap B$ (draw a Venn diagram). Then these three sets are disjoint and their union
is $A \cup B$, so by additivity,
\[
\mathbf{P}(A \cup B) = \mathbf{P}(A_1 ) + \mathbf{P}(B_1 ) + \mathbf{P}(C).
\]
We also have $A = A_1 \cup C$ and $B = B_1 \cup C$ which gives
\[
\mathbf{P}(A) = \mathbf{P}(A_1 ) + \mathbf{P}(C)
\]
and
\[
\mathbf{P}(B) = \mathbf{P}(B_1 ) + \mathbf{P}(C)
\]
Substituting from these two equations into the previous equation for $\mathbf{P}(A \cup B)$ so as to eliminate
$\mathbf{P}(A_1 )$ and $\mathbf{P}(B_1 )$ gives the result.

4. (Taking motivation from definition of $B$ above.) Let $B_1 = A_1$ and then for $k\geq  2$,
\[
B_k = A_k \cap (A_1\cup  A_2 \cup \ldots \cup  A_{k-1} )^c
\]
then $B_1 , B_2 , \ldots$ form a sequence of disjoint events, and $\displaystyle \bigcup_{i=1}^n A_i = \displaystyle \bigcup_{i=1}^n B_i$ , so by additivity
\[
\mathbf{P}\left(
\bigcup_{i=1}^n A_i 
\right)
=
\mathbf{P}\left(
\bigcup_{i=1}^n B_i 
\right)
=
\sum_{i=1}^n \mathbf{P}(B_i) 
\leq 
\sum_{i=1}^n \mathbf{P}(A_i) 
\]
where the last inequality follows by applying the property $A\subseteq B$ then $\mathbf{P}(A) \leq \mathbf{P}(B)$ since $B_i \subseteq A_i$ for every $i$. For an infinite sequence of events the argument is the same.
:::

::::{.example}
Ten players enter a tournament in which they each play each other player once. Assume all outcomes are equally likely and there are no ties. Determine the probability that some player wins all their games?
::::


:::{.solution}
First note that we can count the games that get played by considering the set $\{(i, j) : 1 \leq i < j \leq 10\}$
which has 45 elements; that is, $\frac{1}{2}(9\times 10)$, the 10 players play 9 games because they do not play themselves. A game between player $i$ and player $j$ has two outcomes (we assume no ties
here!) and so the sample space contains $2^{45}$ sample points. 

Let $A_i$ denote for $i = 1, 2, \ldots , 10$, the event
that player $i$ wins all their games. A tournament outcome such that $A_i$ occurs consists 9 games where
the result is determined (player $i$ wins all their games) and 36 games where the result isn't determined. So $|A_i | = 2^{45-9} = 2^{36}$,
and consequently $\mathbf{P}(A_i ) = \dfrac{2^{36}}{2^{45}} = 2^{-9}$ . 

The event that some player wins all their games is
\[
E=
\bigcup_{i=1}^{10}
A_i
\]
and the key observation to make is that $A_i  \cap A_j = \emptyset$ whenever $i \neq j$ because it is impossible for both $i$
and $j$ to win the game that takes place between the two of them. Consequently
\[
\mathbf{P}(E) = \sum_{i=1}^{10} \mathbf{P}(A_i ) = 10\times 2^{-9}.
\]
:::



:::{.example}
Let $\Omega$ be a sample space of an experiment. Suppose $| \Omega | = n$. Suppose for every $\omega_1, \omega_2 \in \Omega$ that events $\{ \omega_1 \}$ and $\{ \omega_2 \}$ are equally likely. Prove using the properties of probability measures that for any $\omega \in \Omega$, $\mathbf{P}(\{\omega\}) = \frac{1}{| \Omega |}$.
::::


:::{.solution}
Write $\Omega = \{ \omega_1, \ldots \omega_n \}$. Then $\Omega = \cup_{i=1}^n \{ \omega_i \}$. The properties of a probability measures imply
\begin{align*}
1 &= \mathbf{P} (\Omega) \\
&= \mathbf{P} \left( \cup_{i=1}^n \{ \omega_i \} \right) \\
&= \sum_{i=1}^n \mathbf{P}( \{ \omega_i \}) \quad \mbox{using additivity} \\
&= n\mathbf{P}( \{ \omega \})
\end{align*}
Here we have used the fact $\mathbf{P}(\{ \omega_1 \})=\mathbf{P}(\{ \omega_i \})$ for $i=1,2,\ldots, n$. Therefore, 
\[
\mathbf{P}( \{ \omega_1 \} ) = \frac{1}{n}.
\]
Whence, $\mathbf{P}( \{ \omega_i \} ) = \mathbf{P}( \{ \omega_1 \} )= \frac{1}{n} = \frac{1}{| \Omega |}$, for $i=1,\ldots n$.

Therefore, $\omega \in \Omega$, $\mathbf{P}(\omega) = \frac{1}{| \Omega |}$.
:::


